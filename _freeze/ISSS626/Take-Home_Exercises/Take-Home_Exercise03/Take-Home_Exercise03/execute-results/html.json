{
  "hash": "80801a0df5584b6e0644773ac4c81edd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Take Home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods\"\nauthor: \"Pelle Knegjes\"\ndate: \"Nov 10 2024\"\ndate-modified: \"last-modified\"\nexecute: \n  eval: true\n  echo: true\n  message: false\n  freeze: true\n---\n\n\n# **Setting the Scene**\n\nHousing is an essential component of household wealth worldwide. Buying a housing has always been a major investment for most people. The price of housing is affected by many factors. Some of them are global in nature such as the general economy of a country or inflation rate. Others can be more specific to the properties themselves. These factors can be further divided to structural and locational factors. Structural factors are variables related to the property themselves such as the size, fitting, and tenure of the property. Locational factors are variables related to the neighbourhood of the properties such as proximity to childcare centre, public transport service and shopping centre.\n\nConventional, housing resale prices predictive models were built by using Ordinary Least Square (OLS) method. However, this method failed to take into consideration that spatial autocorrelation and spatial heterogeneity exist in geographic data sets such as housing transactions. With the existence of spatial autocorrelation, the OLS estimation of predictive housing resale pricing models could lead to biased, inconsistent, or inefficient results (Anselin 1998). In view of this limitation, **Geographical Weighted Models** were introduced to better calibrate predictive models for housing resale prices.\n\n# **The Task**\n\nIn this take-home exercise, you are required to calibrate a predictive model to predict HDB resale prices between July-September 2024 by using HDB resale transaction records in 2023.\n\n# **The Data**\n\nFor the purpose of this take-home exercise, **HDB Resale Flat Prices** provided by [**Data.gov.sg**](https://isss626-ay2024-25aug.netlify.app/take-home_ex03b) should be used as the core data set. The study should focus on either three-room, four-room or five-room flat.\n\nBelow is a list of recommended predictors to consider. However, students are free to include other appropriate independent variables.\n\n-   Structural factors\n\n    -   Area of the unit\n\n    -   Floor level\n\n    -   Remaining lease\n\n    -   Age of the unit\n\n-   Locational factors\n\n    -   Proxomity to CBD\n\n    -   Proximity to eldercare\n\n    -   Proximity to foodcourt/hawker centres\n\n    -   Proximity to MRT\n\n    -   Proximity to park\n\n    -   Proximity to good primary school\n\n    -   Proximity to shopping mall\n\n    -   Proximity to supermarket\n\n    -   Numbers of kindergartens within 350m\n\n    -   Numbers of childcare centres within 350m\n\n    -   Numbers of bus stop within 350m\n\n    -   Numbers of primary school within 1km\n\n# Data Wrangling\n\n## **Package Descriptions**\n\nFirst, we will load in the following packages:\n\n1.  **sf**: Provides support for simple features, enabling the handling of spatial data in R. It allows for easy manipulation and analysis of spatial objects.\n\n2.  **spdep**: Contains functions for spatial dependence and spatial autocorrelation analysis. It is useful for working with spatial data and understanding spatial relationships.\n\n3.  **GWmodel**: Implements geographically weighted regression (GWR) and other geographically weighted models, allowing for the analysis of spatially varying relationships.\n\n4.  **SpatialML**: Provides tools for spatial machine learning, including methods for spatial data analysis and modeling.\n\n5.  **tmap**: A package for thematic mapping in R, allowing for the creation of static and interactive maps.\n\n6.  **rsample**: Provides functions for creating and working with resampling objects, useful for cross-validation and bootstrapping.\n\n7.  **Metrics**: Contains functions for evaluating the performance of regression models, including various metrics like RMSE, MAE, and R-squared.\n\n8.  **tidyverse**: A collection of R packages designed for data science, including tools for data manipulation (dplyr), visualization (ggplot2), and more.\n\n9.  **ClustGeo**: Provides tools for spatial clustering and geostatistical analysis, including methods for clustering spatial data.\n\n10. **ggpubr**: A package that provides easy-to-use functions for creating publication-ready plots with ggplot2, including functions for arranging multiple plots.\n\n11. **cluster**: Contains functions for cluster analysis, including various clustering algorithms and methods for evaluating clustering results.\n\n12. **factoextra**: A package for visualizing and interpreting the results of multivariate data analyses, including clustering and principal component analysis (PCA).\n\n13. **NbClust**: Provides methods for determining the optimal number of clusters in a dataset, offering various indices for cluster validation.\n\n14. **heatmaply**: A package for creating interactive heatmaps in R, useful for visualizing complex data matrices.\n\n15. **corrplot**: Provides functions for visualizing correlation matrices, making it easy to understand relationships between variables.\n\n16. **psych**: Contains functions for psychological research, including tools for descriptive statistics, reliability analysis, and factor analysis.\n\n17. **GGally**: Extends ggplot2 by providing functions for creating a variety of plots, including pair plots and correlation plots.\n\n18. **sfdep**: A package for spatial dependence analysis, providing tools for exploring and modeling spatial relationships.\n\n19. **plotly**: A library for creating interactive plots and dashboards, allowing for dynamic visualizations of data.\n\n20. **Kendall**: Implements functions for calculating Kendall's tau, a measure of correlation between two variables.\n\n21. **SpatialAcc**: Provides tools for assessing the accuracy of spatial predictions, including methods for cross-validation and error analysis.\n\n22. **ggstatsplot**: A package that integrates statistical tests into ggplot2 visualizations, providing informative plots with statistical results.\n\n23. **reshape2**: Provides functions for reshaping data, allowing for easy transformation between wide and long formats.\n\n24. **httr**: A package for working with HTTP requests, making it easier to interact with web APIs and retrieve data from the web.\n\n25. **jsonlite**: Provides functions for converting between R objects and JSON, making it easy to work with JSON data.\n\n26. **rvest**: A package for web scraping, allowing users to extract data from HTML web pages.\n\n27. **olsrr**: Provides tools for building and validating ordinary least squares (OLS) regression models, including stepwise regression and model diagnostics.\n\n28. **gtsummary**: A package for creating summary tables of statistical models, making it easy to present results in a clear format.\n\n29. **performance**: Provides tools for assessing the performance of statistical models, including diagnostics and evaluation metrics.\n\n30. **see**: A package for visualizing statistical results, providing functions for creating informative plots and visual summaries.\n\nThese packages collectively provide a robust toolkit for spatial analysis, data manipulation, visualization, and statistical modeling in R. Depending on your specific analysis needs, you can leverage these packages to perform a wide range of tasks, from data cleaning and exploration to advanced spatial modeling and visualization.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(sf, spdep, GWmodel, SpatialML, \n               tmap, rsample, Metrics, tidyverse, ClustGeo, \n               ggpubr, cluster, factoextra, NbClust, heatmaply, corrplot, psych, GGally, spdep, tmap, sfdep, plotly, Kendall, SpatialAcc, ggstatsplot, reshape2,httr, jsonlite, rvest, olsrr, gtsummary, performance, see, kableExtra, knitr)\n```\n:::\n\n\n## Set Seed\n\nUsing **`set.seed()`** is a best practice in data analysis and statistical modeling when randomness is involved. It helps ensure that your results can be reproduced and verified, which is essential for scientific rigor and transparency.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n```\n:::\n\n\n## More data Wrangling\n\nThe next code snippet reads the Singapore subzone dataset, and then transforms the coordinate reference system of that dataset to a new CRS (3414).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpsz = st_read(dsn = \"data/geospatial\", layer = \"MP14_SUBZONE_WEB_PL\")%>%\n  st_transform(crs = 3414)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\BlumeTechnologies\\ISSS626\\ISSS626\\Take-Home_Exercises\\Take-Home_Exercise03\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n```\n\n\n:::\n:::\n\n\n**Read CSV File**:\n\nFirst we read a CSV file containing resale flat prices from January 2017 onwards using **`read_csv()`**. The file is located at **`\"data/aspatial/ResaleflatpricesbasedonregistrationdatefromJan-2017onwards.csv\"`**.\n\n**Filter Data**:\n\nThen we filter the dataset to include only records where the **`month`** is between January 2023 (**`\"2023-01\"`**) and September 2024 (**`\"2024-09\"`**).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresale <- read_csv(\"data/aspatial/ResaleflatpricesbasedonregistrationdatefromJan-2017onwards.csv\") %>%\n  filter(month >= \"2023-01\" & month <= \"2024-09\")\n```\n:::\n\n\n**Data Transformation**:\n\nWe create a new column **`address`** by concatenating the **`block`** and **`street_name`** columns.\n\nWe extract the first two characters from the **`remaining_lease`** column to create a new integer column **`remaining_lease_yr`**, representing the remaining lease in years.\n\nWe extract the characters from positions 9 to 11 of the **`remaining_lease`** column to create another integer column **`remaining_lease_mth`**, representing the remaining lease in months.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresale_tidy <- resale %>%\n  mutate(address = paste(block,street_name)) %>%\n  mutate(remaining_lease_yr = as.integer(\n    str_sub(remaining_lease, 0, 2)))%>%\n  mutate(remaining_lease_mth = as.integer(\n    str_sub(remaining_lease, 9, 11)))\n```\n:::\n\n\n**Save Tidy Data**:\n\nFinally, we save the transformed dataset (**`resale_tidy`**) as an RDS file at the specified path **`\"data/rds/resale_tidy.rds\"`** using **`write_rds()`**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(resale_tidy, \"data/rds/resale_tidy.rds\")\n```\n:::\n\n\n**Select Resale Data**:\n\nWe start by creating a new variable **`resale_selected`** that holds the data from **`resale_tidy`**. This means we are working with a tidy version of the resale flat price data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresale_selected <- resale_tidy\n```\n:::\n\n\n**Create a Unique Address List**:\n\nNext, we generate a sorted list of unique addresses from the **`address`** column in **`resale_selected`**. This list will be used to query the API for geographic coordinates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadd_list <- sort(unique(resale_selected$address))\n```\n:::\n\n\n**Define a Function to Get Coordinates**:\n\n-   We define a function called **`get_coords`** that takes the list of addresses (**`add_list`**) as input. This function will retrieve the postal codes, latitude, and longitude for each address using the OneMap API.\n\n**Loop Through Each Address**:\n\n-   Inside the function, we loop through each address in **`add_list`**. For each address:\n\n    -   We make a GET request to the OneMap API to search for the address.\n\n    -   We parse the JSON response to check how many results were found.\n\n**Handle API Response**:\n\n-   If one result is found, we extract the postal code, latitude, and longitude and create a new row for the results.\n\n-   If multiple results are found, we filter out any results with \"NIL\" as the postal code and take the first valid result.\n\n-   If no results are found, we create a row with **`NA`** values for postal code, latitude, and longitude.\n\n**Store Results**:\n\nEach new row of results is appended to the **`postal_coords`** data frame, which accumulates the coordinates for all addresses.\n\n**Return Coordinates Data Frame**:\n\nAfter processing all addresses, the function returns the **`postal_coords`** data frame containing the addresses along with their corresponding postal codes, latitudes, and longitudes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_coords <- function(add_list){\n  \n  # Create a data frame to store all retrieved coordinates\n  postal_coords <- data.frame()\n    \n  for (i in add_list){\n    #print(i)\n\n    r <- GET('https://www.onemap.gov.sg/api/common/elastic/search?',\n           query=list(searchVal=i,\n                     returnGeom='Y',\n                     getAddrDetails='Y'))\n    data <- fromJSON(rawToChar(r$content))\n    found <- data$found\n    res <- data$results\n    \n    # Create a new data frame for each address\n    new_row <- data.frame()\n    \n    # If single result, append \n    if (found == 1){\n      postal <- res$POSTAL \n      lat <- res$LATITUDE\n      lng <- res$LONGITUDE\n      new_row <- data.frame(address= i, \n                            postal = postal, \n                            latitude = lat, \n                            longitude = lng)\n    }\n    \n    # If multiple results, drop NIL and append top 1\n    else if (found > 1){\n      # Remove those with NIL as postal\n      res_sub <- res[res$POSTAL != \"NIL\", ]\n      \n      # Set as NA first if no Postal\n      if (nrow(res_sub) == 0) {\n          new_row <- data.frame(address= i, \n                                postal = NA, \n                                latitude = NA, \n                                longitude = NA)\n      }\n      \n      else{\n        top1 <- head(res_sub, n = 1)\n        postal <- top1$POSTAL \n        lat <- top1$LATITUDE\n        lng <- top1$LONGITUDE\n        new_row <- data.frame(address= i, \n                              postal = postal, \n                              latitude = lat, \n                              longitude = lng)\n      }\n    }\n\n    else {\n      new_row <- data.frame(address= i, \n                            postal = NA, \n                            latitude = NA, \n                            longitude = NA)\n    }\n    \n    # Add the row\n    postal_coords <- rbind(postal_coords, new_row)\n  }\n  return(postal_coords)\n}\n```\n:::\n\n\n**Call the Function**:\n\nWe call the **`get_coords`** function with **`add_list`** to retrieve the coordinates and store the results in the variable **`coords`**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoords <- get_coords(add_list)\n```\n:::\n\n\n**Save Coordinates to RDS File**:\n\nWe save the **`coords`** data frame to an RDS file at the specified path **`\"data/rds/coords.rds\"`** for future use.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(coords, \"data/rds/coords.rds\")\n```\n:::\n\n\n**Read Coordinates from RDS File**:\n\nWe read the saved coordinates from the RDS file back into the variable **`coords`**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoords <- read_rds(\"data/rds/coords.rds\")\n```\n:::\n\n\n**Read Tidy Resale Data from RDS File**:\n\nFinally, we read the tidy resale data from the RDS file back into the variable **`resale_tidy`**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresale_tidy <- read_rds(\"data/rds/resale_tidy.rds\")\n```\n:::\n\n\n**Join Resale Data with Coordinates**:\n\nWe start by merging the **`resale_tidy`** dataset with the **`coords`** dataset using a left join. This combines the resale flat data with the corresponding geographic coordinates (latitude and longitude).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresaleRAW <- left_join(resale_tidy, coords)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nJoining with `by = join_by(address, postal)`\n```\n\n\n:::\n:::\n\n\n**Convert to Spatial Data Frame**:\n\nNext, we convert the merged dataset (**`resaleRAW`**) into a spatial data frame using **`st_as_sf()`**. We specify the columns for longitude and latitude, and set the coordinate reference system (CRS) to WGS 84 (EPSG:4326).\n\nAfter that, we transform the CRS to a different projection (EPSG:3414) and apply a jitter effect to the spatial points to add a small random variation (0.5 units) to their positions. This is often done to avoid overlapping points in visualizations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresale <- st_as_sf(resaleRAW, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %>%\n  st_transform(crs = 3414)%>%\n  st_jitter(amount = 0.5)\n```\n:::\n\n\n**Define a Function to Assign Dummy Variables**:\n\n-   We define a function called **`assign_dummy`** that takes a **`range`** as input and assigns a dummy variable based on predefined ranges of storey levels. The function categorizes the ranges into three groups:\n\n    -   1 for \"Low\" (ranges 01 to 18)\n\n    -   2 for \"Medium\" (ranges 19 to 36)\n\n    -   3 for \"High\" (any other range).\n\n**Apply the Function to Create a New Column**:\n\nWe use the **`sapply()`** function to apply the **`assign_dummy`** function to the **`storey_range`** column of the **`resale`** dataset. This creates a new column called **`floor_level`** that contains the assigned dummy values based on the storey ranges.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nassign_dummy <- function(range) {\n  if (range %in% c(\"01 TO 03\", \"04 TO 06\", \"07 TO 09\", \"10 TO 12\", \n                   \"13 TO 15\", \"16 TO 18\")) {\n    return(1)  # Low\n  } else if (range %in% c(\"19 TO 21\", \"22 TO 24\", \"25 TO 27\", \n                          \"28 TO 30\", \"31 TO 33\", \"34 TO 36\")) {\n    return(2)  # Medium\n  } else {\n    return(3)  # High\n  }\n}\n\n# Apply the function to create a new column in the resale data frame\nresale$floor_level <- sapply(resale$storey_range, assign_dummy)\n```\n:::\n\n\n**Save the Final Resale Data**:\n\nWe save the final spatial dataset (**`resale`**) with the new **`floor_level`** column to an RDS file at the specified path **`\"data/rds/resale.rds\"`** for future use.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(resale, \"data/rds/resale.rds\")\n```\n:::\n\n\n**Read the Resale Data from RDS File**:\n\nFinally, we read the saved resale data from the RDS file back into the variable **`resale`**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresale <- read_rds(\"data/rds/resale.rds\")\n```\n:::\n\n\n## Creating Buffer Zones\n\n**Create a 350-Meter Buffer**:\n\nWe use the **`st_buffer()`** function from the **`sf`** package to create a buffer zone of 350 meters around each spatial point in the **`resale`** dataset. This function generates a new spatial object that represents the area within 350 meters of each point.\n\n**Create a 1000-Meter Buffer**:\n\nSimilarly, we create another buffer zone of 1000 meters around each spatial point in the **`resale`** dataset using the same **`st_buffer()`** function. This generates a new spatial object that represents the area within 1000 meters of each point.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbuffer350 = st_buffer(resale, dist = 350)\nbuffer1000 = st_buffer(resale, dist = 1000)\n```\n:::\n\n\n## Loading in location predictor variables\n\nIn this section we load in the location predictor variables aswell as calculating and finding distances\n\nWe use the **`st_distance()`** function from the **`sf`** package to compute the distances between each point in the **`resale`** dataset and the points in the **`CBD`** dataset. This function returns a distance matrix where each entry represents the distance between a point in **`resale`** and a point in the predictor value.\n\nWe apply the **`apply()`** function to the distance matrix to find the minimum distance for each point in the **`resale`** dataset. The **`1`** argument indicates that we are applying the function across rows (i.e., for each point in **`resale`**). The result is a vector of minimum distances.\n\n<!-- -->\n\nThe **`st_intersects()`** function from the **`sf`** package is used to determine which which locations fall within the 350 or 1000-meter buffer zones created earlier (stored in **`buffer350/1000`**). This function returns a list where each element corresponds to a buffer zone and contains the indices of locations that intersect with that zone.\n\n<!-- -->\n\nThe **`lengths()`** function is then applied to the list of intersections obtained from **`st_intersects()`**. This function counts the number of locations that intersect with each buffer zone. The result is a vector where each element represents the count of locations within the corresponding buffer zone.\n\n::: panel-tabset\n### Proximity to CBD\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCBD <- data.frame(\n  longitude = c(103.8503),  # Example longitudes\n  latitude = c(1.2812)      # Example latitudes\n)\n\nCBD <- st_as_sf(CBD, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %>%\n  st_transform(crs = 3414)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndistances = st_distance(resale, CBD)\nmin_distances <- apply(distances, 1, min)\nresale$PROX_CBD = min_distances\n```\n:::\n\n\n### Proximity to Eldercare\n\n\n::: {.cell}\n\n```{.r .cell-code}\neldercare <- st_read(dsn = \"data/geospatial\", layer = \"ELDERCARE\") %>%\n  st_transform(crs = 3414) \n\ndistances = st_distance(resale, eldercare)\n\nmin_distances <- apply(distances, 1, min)\nresale$PROX_ELDERLYCARE = min_distances\n```\n:::\n\n\n### Proximity to Hawker centres\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhawker <- st_read(\"data/geospatial/HawkerCentresGEOJSON.geojson\") %>%\n  st_transform(crs = 3414) \n\ndistances = st_distance(resale, hawker)\n\nmin_distances <- apply(distances, 1, min)\nresale$PROX_HAWKER = min_distances\n```\n:::\n\n\n### Proximity to MRT\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmrt <- read_csv(\"data/aspatial/MRT.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmrt <- st_as_sf(mrt, \n                       coords = c(\"Longitude\", \"Latitude\"),\n                       crs=4326) %>%\n  st_transform(crs = 3414)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndistances = st_distance(resale, mrt)\n\nmin_distances <- apply(distances, 1, min)\nresale$PROX_MRT = min_distances\n```\n:::\n\n\n### Proximity to Park\n\n\n::: {.cell}\n\n```{.r .cell-code}\npark <- st_read(\"data/geospatial/Parks.geojson\")  %>%\n  st_transform(crs = 3414)\n\ndistances = st_distance(resale, park)\n\nmin_distances <- apply(distances, 1, min)\nresale$PROX_PARK = min_distances\n```\n:::\n\n\n### Proximity to Shopping Mall\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmalls <- read_csv(\"data/aspatial/shopping_mall_coordinates.csv\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmalls <- st_as_sf(malls, \n                       coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                       crs=4326) %>%\n  st_transform(crs = 3414)\n\ndistances = st_distance(resale, malls)\n\nmin_distances <- apply(distances, 1, min)\nresale$PROX_MALL = min_distances\n```\n:::\n\n\n### Proximity to Supermarket\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsupermarket <- st_read(\"data/geospatial/SupermarketsGEOJSON.geojson\") %>%\n  st_transform(crs = 3414) \n\ndistances = st_distance(resale, supermarket)\n\nmin_distances <- apply(distances, 1, min)\nresale$PROX_SUPERMARKET = min_distances\n```\n:::\n\n\n### Number of Kidergartens within 350m\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkindergarten <- st_read(\"data/geospatial/Kindergartens.geojson\") %>%\n  st_transform(crs = 3414)\n\nresale$WITHIN_350M_KINDERGARTEN = lengths(\n  st_intersects(buffer350, kindergarten)\n)\n```\n:::\n\n\n### Number of childcare centres within 350m\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchildcare <- st_read(\"data/geospatial/ChildCareServices.geojson\") %>%\n  st_transform(crs = 3414) \n\n\nresale$WITHIN_350M_CHILDCARE = lengths(\n  st_intersects(buffer350, childcare)\n)\n```\n:::\n\n\n### Numbers of bus stop within 350m\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbusstop <- st_read(dsn = \"data/geospatial\", layer = \"BusStop\") %>%\n  st_transform(crs = 3414) \n\nresale$WITHIN_350M_BUS = lengths(\n  st_intersects(buffer350, busstop)\n)\n```\n:::\n\n\n### Numbers of primary school within 1km\n\n\n::: {.cell}\n\n```{.r .cell-code}\nschools <- read_csv(\"data/aspatial/Generalinformationofschools.csv\") %>%\n  filter(mainlevel_code == \"PRIMARY\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nadd_list1 <- sort(unique(schools$address))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nget_coords1 <- function(add_list1){\n  \n  # Create a data frame to store all retrieved coordinates\n  postal_coords1 <- data.frame()\n    \n  for (i in add_list1){\n    #print(i)\n\n    r <- GET('https://www.onemap.gov.sg/api/common/elastic/search?',\n           query=list(searchVal=i,\n                     returnGeom='Y',\n                     getAddrDetails='Y'))\n    data <- fromJSON(rawToChar(r$content))\n    found <- data$found\n    res <- data$results\n    \n    # Create a new data frame for each address\n    new_row <- data.frame()\n    \n    # If single result, append \n    if (found == 1){\n      postal <- res$POSTAL \n      lat <- res$LATITUDE\n      lng <- res$LONGITUDE\n      new_row <- data.frame(address= i, \n                            postal = postal, \n                            latitude = lat, \n                            longitude = lng)\n    }\n    \n    # If multiple results, drop NIL and append top 1\n    else if (found > 1){\n      # Remove those with NIL as postal\n      res_sub <- res[res$POSTAL != \"NIL\", ]\n      \n      # Set as NA first if no Postal\n      if (nrow(res_sub) == 0) {\n          new_row <- data.frame(address= i, \n                                postal = NA, \n                                latitude = NA, \n                                longitude = NA)\n      }\n      \n      else{\n        top1 <- head(res_sub, n = 1)\n        postal <- top1$POSTAL \n        lat <- top1$LATITUDE\n        lng <- top1$LONGITUDE\n        new_row <- data.frame(address= i, \n                              postal = postal, \n                              latitude = lat, \n                              longitude = lng)\n      }\n    }\n\n    else {\n      new_row <- data.frame(address= i, \n                            postal = NA, \n                            latitude = NA, \n                            longitude = NA)\n    }\n    \n    # Add the row\n    postal_coords1 <- rbind(postal_coords1, new_row)\n  }\n  return(postal_coords1)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncoordsschool <- get_coords(add_list1)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(coordsschool, \"data/rds/coordsschool.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncoordsschool <- read_rds(\"data/rds/coordsschool.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nschools <- left_join(schools, coordsschool)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nschools <- st_as_sf(schools, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %>%\n  st_transform(crs = 3414)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresale$WITHIN_1KM_SCHOOL = lengths(\n  st_intersects(buffer1000, schools)\n)\n```\n:::\n\n\n### Proximity to Good Primary School\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngoodschools = c(\"AI TONG SCHOOL\", \"ANGLO-CHINESE SCHOOL (JUNIOR)\", \"ANGLO-CHINESE SCHOOL (PRIMARY)\",\"CATHOLIC HIGH SCHOOL (PRIMARY SECTION)\", \"CHIJ ST. NICHOLAS GIRLS’ SCHOOL (PRIMARY SECTION)\", \"CHONGFU SCHOOL\",\n\"FAIRFIELD METHODIST SCHOOL (PRIMARY)\", \"GONGSHANG PRIMARY SCHOOL\",\n\"HENRY PARK PRIMARY SCHOOL\", \"HOLY INNOCENTS' PRIMARY SCHOOL\",\n\"HORIZON PRIMARY SCHOOL\", \"METHODIST GIRLS' SCHOOL (PRIMARY)\",\"NAN HUA PRIMARY SCHOOL\", \"NANYANG PRIMARY SCHOOL\",\n\"NORTHLAND PRIMARY SCHOOL\", \"PEI CHUN PUBLIC SCHOOL\",\n\"PEI HWA PRESBYTERIAN PRIMARY SCHOOL\", \"RED SWASTIKA SCHOOL\",\n\"ROSYTH SCHOOL\", \"RULANG PRIMARY SCHOOL\", \"SOUTH VIEW PRIMARY SCHOOL\", \"ST. HILDA'S PRIMARY SCHOOL\", \"ST. JOSEPH'S INSTITUTION JUNIOR\", \"TAO NAN SCHOOL\", \"TEMASEK PRIMARY SCHOOL\")\n\ngood_schools = schools %>%\n  filter(school_name %in% goodschools)\n\n\ndistances = st_distance(resale, good_schools)\n\nmin_distances <- apply(distances, 1, min)\nresale$PROX_GOOD_PRISCH = min_distances\n```\n:::\n\n:::\n\n## Tidying up dataset\n\nIn this section, we are tidying the resale dataset by selecting specific columns of interest from the original **`resale`** dataset. The **`select()`** function from the **`dplyr`** package allows us to choose only the relevant variables that we want to keep for our analysis. The selected columns include:\n\n-   **resale_price**: The price at which the property was resold.\n\n-   **floor_area_sqm**: The area of the property in square meters.\n\n-   **floor_level**: The level of the floor on which the property is located.\n\n-   **remaining_lease_yr**: The number of years remaining on the lease.\n\n-   **PROX_CBD**: Proximity to the Central Business District.\n\n-   **PROX_ELDERLYCARE**: Proximity to elderly care facilities.\n\n-   **PROX_HAWKER**: Proximity to hawker centers.\n\n-   **PROX_MRT**: Proximity to Mass Rapid Transit stations.\n\n-   **PROX_PARK**: Proximity to parks.\n\n-   **PROX_GOOD_PRISCH**: Proximity to good primary schools.\n\n-   **PROX_MALL**: Proximity to shopping malls.\n\n-   **PROX_SUPERMARKET**: Proximity to supermarkets.\n\n-   **WITHIN_350M_KINDERGARTEN**: Indicator for being within 350 meters of a kindergarten.\n\n-   **WITHIN_350M_CHILDCARE**: Indicator for being within 350 meters of childcare facilities.\n\n-   **WITHIN_350M_BUS**: Indicator for being within 350 meters of a bus stop.\n\n-   **WITHIN_1KM_SCHOOL**: Indicator for being within 1 kilometer of a school.\n\n-   **month**: The month of the transaction.\n\n-   **flat_type**: The type of flat (e.g., HDB, private).\n\n-   **flat_model**: The model of the flat.\n\n-   **storey_range**: The range of storeys for the flat.\n\n-   **lease_commence_date**: The date when the lease commenced.\n\n-   **remaining_lease_mth**: The number of months remaining on the lease.\n\n-   **address**: The address of the property.\n\n-   **remaining_lease**: The remaining lease period.\n\n-   **postal**: The postal code.\n\n-   **geometry**: The spatial geometry of the property.\n\nBy selecting these columns, we ensure that our dataset is focused on the variables that are relevant for our analysis, particularly for modeling resale prices.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresale_tidy = resale %>%\n  select(resale_price, floor_area_sqm, floor_level,  remaining_lease_yr,PROX_CBD, PROX_ELDERLYCARE, PROX_HAWKER, PROX_MRT, PROX_PARK, PROX_GOOD_PRISCH, PROX_MALL, PROX_SUPERMARKET, WITHIN_350M_KINDERGARTEN, WITHIN_350M_CHILDCARE, WITHIN_350M_BUS, WITHIN_1KM_SCHOOL,month, flat_type,flat_model, storey_range, lease_commence_date, remaining_lease_mth, address, remaining_lease, ,postal, geometry)\n```\n:::\n\n\nIn this line, we save the tidied dataset (**`resale_tidy`**) to an RDS file using the **`write_rds()`** function. This allows us to store the cleaned and selected data for future use without needing to repeat the tidying process. The **`eval: false`** comment indicates that this code should not be executed in the current context, possibly to avoid overwriting existing files.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(resale_tidy, \"data/rds/resale_tidy.rds\")\n```\n:::\n\n\nHere, we load the previously saved tidied dataset from the RDS file back into our R environment using the **`read_rds()`** function. This allows us to access the cleaned data for further analysis or modeling.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresale_tidy <- read_rds(\"data/rds/resale_tidy.rds\")\n```\n:::\n\n\nIn this final step, we generate a summary of the **`resale_tidy`** dataset using the **`summary()`** function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(resale_tidy)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  resale_price     floor_area_sqm    floor_level   remaining_lease_yr\n Min.   : 150000   Min.   : 31.00   Min.   :1.00   Min.   :41.0      \n 1st Qu.: 460000   1st Qu.: 74.00   1st Qu.:1.00   1st Qu.:60.0      \n Median : 565000   Median : 93.00   Median :1.00   Median :73.0      \n Mean   : 587542   Mean   : 95.22   Mean   :1.06   Mean   :73.3      \n 3rd Qu.: 685000   3rd Qu.:111.00   3rd Qu.:1.00   3rd Qu.:90.0      \n Max.   :1588000   Max.   :366.70   Max.   :3.00   Max.   :97.0      \n                                                                     \n    PROX_CBD       PROX_ELDERLYCARE    PROX_HAWKER          PROX_MRT      \n Min.   :  582.9   Min.   :   0.027   Min.   :   6.719   Min.   :  21.71  \n 1st Qu.:10088.2   1st Qu.: 330.194   1st Qu.: 357.323   1st Qu.: 338.70  \n Median :13793.4   Median : 630.634   Median : 637.947   Median : 559.70  \n Mean   :12788.1   Mean   : 798.225   Mean   : 752.096   Mean   : 632.29  \n 3rd Qu.:15608.0   3rd Qu.:1092.804   3rd Qu.:1002.451   3rd Qu.: 831.20  \n Max.   :20167.8   Max.   :4767.706   Max.   :2868.089   Max.   :3491.85  \n                                                                          \n   PROX_PARK      PROX_GOOD_PRISCH    PROX_MALL       PROX_SUPERMARKET  \n Min.   :  46.1   Min.   :  49.52   Min.   :   0.17   Min.   :   0.042  \n 1st Qu.: 456.1   1st Qu.: 919.27   1st Qu.: 391.83   1st Qu.: 176.204  \n Median : 671.7   Median :1663.19   Median : 612.04   Median : 273.681  \n Mean   : 772.5   Mean   :1911.82   Mean   : 680.68   Mean   : 294.183  \n 3rd Qu.: 982.5   3rd Qu.:2539.93   3rd Qu.: 909.26   3rd Qu.: 385.881  \n Max.   :2412.0   Max.   :7564.48   Max.   :3203.02   Max.   :3325.498  \n                                                                        \n WITHIN_350M_KINDERGARTEN WITHIN_350M_CHILDCARE WITHIN_350M_BUS \n Min.   :0.0000           Min.   : 0.00         Min.   : 0.000  \n 1st Qu.:0.0000           1st Qu.: 3.00         1st Qu.: 6.000  \n Median :1.0000           Median : 4.00         Median : 8.000  \n Mean   :0.9786           Mean   : 4.63         Mean   : 7.912  \n 3rd Qu.:1.0000           3rd Qu.: 6.00         3rd Qu.:10.000  \n Max.   :8.0000           Max.   :22.00         Max.   :19.000  \n                                                                \n WITHIN_1KM_SCHOOL    month            flat_type          flat_model       \n Min.   :0.000     Length:47428       Length:47428       Length:47428      \n 1st Qu.:2.000     Class :character   Class :character   Class :character  \n Median :3.000     Mode  :character   Mode  :character   Mode  :character  \n Mean   :2.976                                                             \n 3rd Qu.:4.000                                                             \n Max.   :9.000                                                             \n                                                                           \n storey_range       lease_commence_date remaining_lease_mth   address         \n Length:47428       Min.   :1966        Min.   : 1.000      Length:47428      \n Class :character   1st Qu.:1985        1st Qu.: 3.000      Class :character  \n Mode  :character   Median :1998        Median : 6.000      Mode  :character  \n                    Mean   :1998        Mean   : 6.077                        \n                    3rd Qu.:2015        3rd Qu.: 9.000                        \n                    Max.   :2022        Max.   :11.000                        \n                                        NA's   :3951                          \n remaining_lease       postal                   geometry    \n Length:47428       Length:47428       POINT        :47428  \n Class :character   Class :character   epsg:3414    :    0  \n Mode  :character   Mode  :character   +proj=tmer...:    0  \n                                                            \n                                                            \n                                                            \n                                                            \n```\n\n\n:::\n:::\n\n\n# Exploratory Data Analysis (EDA)\n\n## EDA using Statistical Graphics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data=resale_tidy, aes(x=`resale_price`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n```\n\n::: {.cell-output-display}\n![](Take-Home_Exercise03_files/figure-html/unnamed-chunk-48-1.png){width=672}\n:::\n:::\n\n\nThe figure above reveals a slight right skewed distribution. \\## Multiple Histogram Plots Distribution of Variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfloor_area_sqm <- ggplot(data=resale_tidy, aes(x= `floor_area_sqm`)) + \n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nfloor_level <- ggplot(data=resale_tidy, aes(x= `floor_level`)) +\n  geom_histogram(bins=3, color=\"black\", fill=\"light blue\")\n\nremaining_lease_yr <- ggplot(data=resale_tidy, aes(x= `remaining_lease_yr`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_CBD <- ggplot(data=resale_tidy, aes(x= `PROX_CBD`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_ELDERLYCARE <- ggplot(data=resale_tidy, aes(x= `PROX_ELDERLYCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_HAWKER <- ggplot(data=resale_tidy, aes(x= `PROX_HAWKER`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_MRT <- ggplot(data=resale_tidy, aes(x= `PROX_MRT`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_PARK <- ggplot(data=resale_tidy, aes(x= `PROX_PARK`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\nPROX_GOOD_PRISCH <- ggplot(data=resale_tidy, aes(x= `PROX_GOOD_PRISCH`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\nPROX_MALL <- ggplot(data=resale_tidy, aes(x= `PROX_MALL`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nPROX_SUPERMARKET <- ggplot(data=resale_tidy, aes(x= `PROX_SUPERMARKET`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\nWITHIN_350M_KINDERGARTEN <- ggplot(data=resale_tidy, aes(x= `WITHIN_350M_KINDERGARTEN`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nWITHIN_350M_CHILDCARE <- ggplot(data=resale_tidy, aes(x= `WITHIN_350M_CHILDCARE`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nWITHIN_350M_BUS <- ggplot(data=resale_tidy, aes(x= `WITHIN_350M_BUS`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\nWITHIN_1KM_SCHOOL <- ggplot(data=resale_tidy, aes(x= `WITHIN_1KM_SCHOOL`)) +\n  geom_histogram(bins=20, color=\"black\", fill=\"light blue\")\n\n\n\n\nggarrange(floor_area_sqm,floor_level,remaining_lease_yr,PROX_CBD,PROX_ELDERLYCARE,PROX_HAWKER,PROX_MRT,PROX_PARK,PROX_GOOD_PRISCH,PROX_MALL,PROX_SUPERMARKET,WITHIN_350M_KINDERGARTEN,WITHIN_350M_CHILDCARE,WITHIN_350M_BUS,WITHIN_1KM_SCHOOL,ncol = 3, nrow = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$`1`\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Take-Home_Exercise03_files/figure-html/unnamed-chunk-49-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n$`2`\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Take-Home_Exercise03_files/figure-html/unnamed-chunk-49-2.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nattr(,\"class\")\n[1] \"list\"      \"ggarrange\"\n```\n\n\n:::\n:::\n\n\n# Data Sampling\n\n## Creating subsets\n\n**Filter for Data from the Year 2023**:\n\n-   The first part of the code uses the **`dplyr`** package to filter the **`resale_tidy`** dataset for records from the year 2023. The **`filter()`** function is used in conjunction with **`grepl()`** to match any month that starts with \"2023-\", and it also filters for specific flat types: \"3 ROOM\", \"4 ROOM\", and \"5 ROOM\".\n\n**Filter for Data Between July and September 2024**:\n\n-   The second part of the code creates another subset of the **`resale_tidy`** dataset, this time filtering for records between July and September of 2024. It uses the **`filter()`** function to select rows where the **`month`** is either \"2024-07\", \"2024-08\", or \"2024-09\", and again filters for the same flat types: \"3 ROOM\", \"4 ROOM\", and \"5 ROOM\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Filter for data from the year 2023\nsubset_2023 <- resale_tidy %>%\n  filter(grepl(\"^2023-\", month) & flat_type %in% c(\"3 ROOM\", \"4 ROOM\", \"5 ROOM\"))\n\n# Filter for data between July and September 2024\nsubset_jul_sep_2024 <- resale_tidy %>%\n  filter(month %in% c(\"2024-07\", \"2024-08\", \"2024-09\") & flat_type %in% c(\"3 ROOM\", \"4 ROOM\", \"5 ROOM\"))\n```\n:::\n\n\n**Save the Subset to an RDS File**:\n\n-   The **`write_rds()`** function from the **`readr`** package is used to save the **`subset_jul_sep_2024`** dataset to a file in RDS format. The file is saved in the \"data/rds/\" directory with the name \"test_data.rds\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(subset_jul_sep_2024, \"data/rds/test_data.rds\")\n```\n:::\n\n\n**Read the RDS File Back into R**:\n\n-   Finally, the **`read_rds()`** function is used to read the saved RDS file back into R, creating a new object called **`test_data`**. This allows you to access the filtered dataset after it has been saved.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_data <- read_rds(\"data/rds/test_data.rds\")\n```\n:::\n\n\n## Creating training data\n\nDue to processing limitations we will need to reduce the number of observations in our training dataset.\n\n**Check Original Size of the Dataset**:\n\n-   First, we calculate the number of rows (observations) in the **`subset_2023`** dataset using the **`nrow()`** function. We store this value in the variable **`original_size`**, which helps us understand the size of the dataset before we proceed with sampling.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noriginal_size <- nrow(subset_2023)\noriginal_size\n```\n:::\n\n\n**Define Sample Size**:\n\n-   Next, we define a variable called **`sample_size`** and set it to 5000. This variable indicates the number of observations we want to randomly select from the **`subset_2023`** dataset to create our training dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_size <- 5000\n```\n:::\n\n\n**Random Sampling**:\n\n-   We then use the **`sample_n()`** function from the **`dplyr`** package to randomly select 5000 observations from the **`subset_2023`** dataset. To ensure that our random selection is reproducible, we call **`set.seed(1234)`** before sampling. We also specify **`replace = FALSE`** to indicate that we do not want to select the same observation more than once.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\ntrain_data <- subset_2023 %>%\n    sample_n(size = sample_size, replace = FALSE)\n```\n:::\n\n\n**Save the Training Data to an RDS File**:\n\n-   After creating our training dataset, we use the **`write_rds()`** function from the **`readr`** package to save the **`train_data`** dataset to a file in RDS format. We save this file in the \"data/rds/\" directory with the name \"train_data.rds\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(train_data, \"data/rds/train_data.rds\")\n```\n:::\n\n\n**Read the RDS File Back into R**:\n\n-   Finally, we read the saved RDS file back into R using the **`read_rds()`** function, creating a new object called **`train_data`**. This allows us to access the randomly sampled training dataset after it has been saved.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_data <- read_rds(\"data/rds/train_data.rds\")\n```\n:::\n\n\n# Computing Correlation Matrix\n\nBefore loading the predictors into a predictive model, it is always a good practice to use correlation matrix to examine if there is sign of multicolinearity.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnogeo_2023 <- train_data %>%\n  st_drop_geometry()\ncorrplot::corrplot(cor(nogeo_2023[, 1:16]), \n                   diag = FALSE, \n                   order = \"AOE\",\n                   tl.pos = \"td\", \n                   tl.cex = 0.5, \n                   method = \"number\", \n                   type = \"upper\")\n```\n\n::: {.cell-output-display}\n![](Take-Home_Exercise03_files/figure-html/unnamed-chunk-58-1.png){width=1152}\n:::\n\n```{.r .cell-code}\ndata_nogeo <- train_data %>%\n  st_drop_geometry()\nggstatsplot::ggcorrmat(data_nogeo[, 2:17])\n```\n\n::: {.cell-output-display}\n![](Take-Home_Exercise03_files/figure-html/unnamed-chunk-58-2.png){width=1152}\n:::\n:::\n\n\nThe correlation matrix above shows that all the correlation values are below 0.8. Hence, there is no sign of multicolinearity.\n\n# Building a Pricing Model by using Multiple Linear Regression Method\n\n## Building a non-spatial multiple linear regression\n\nThe code chunk below using lm() to calibrate the multiple linear regression model.\n\nIn this section, we fit a Multiple Linear Regression (MLR) model to predict the resale_price based on various predictors, including floor_area_sqm, floor_level, and proximity to various amenities. We use the lm() function to create the model and then call summary(resale_mlr) to display the model's summary statistics, which helps us understand the model's performance and the significance of the predictors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresale_mlr <- lm(formula = resale_price ~ floor_area_sqm + floor_level + remaining_lease_yr + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +PROX_MRT + PROX_PARK + PROX_GOOD_PRISCH + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_SCHOOL,\n                  data=train_data)\nsummary(resale_mlr)\n```\n:::\n\n\nHere, we save the fitted MLR model to a file in RDS format using the write_rds() function. This allows us to easily load the model later without needing to refit it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(resale_mlr, \"data/rds/resale_mlr.rds\" ) \n```\n:::\n\n\nIn this step, we load the previously saved MLR model from the RDS file using the read_rds() function. This enables us to use the model for making predictions without having to refit it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresale_mlr <- read_rds(\"data/rds/resale_mlr.rds\")\n```\n:::\n\n\nWe use the fitted MLR model to make predictions on a new dataset, test_data, by calling the predict() function. This generates predicted resale prices based on the features in the test dataset.\n\nWe calculate the Root Mean Square Error (RMSE) to assess the model's prediction accuracy. We first extract the actual resale prices from the test data, then compute the RMSE by taking the square root of the average of the squared differences between the predicted and actual values. Finally, we print the RMSE value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make predictions on the test data\npredictions_mlr <- predict(resale_mlr, newdata=test_data)\n\n# Calculate RMSE\nactuals_mlr <- test_data$resale_price\nrmse_mlr <- sqrt(mean((predictions_mlr - actuals_mlr)^2))\n\n# Print RMSE\nprint(paste(\"Root Mean Square Error:\", rmse_mlr))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Root Mean Square Error: 93308.4371374501\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Get the summary of the model\nsummary_mlr <- summary(resale_mlr)\n\n# Extract R-squared\nr_squared_mlr <- summary_mlr$r.squared\n\n# Print R-squared\nprint(paste(\"R-squared for MLR:\", r_squared_mlr))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"R-squared for MLR: 0.807920494134166\"\n```\n\n\n:::\n\n```{.r .cell-code}\ncombined_df_mlr <- data.frame(predictions_mlr,\n                           actuals_mlr)\n```\n:::\n\n\nWe obtain the summary of the fitted MLR model again to extract the R-squared value, which indicates the proportion of variance in the resale prices explained by the model. We then print this value.And we create a new data frame that combines the predicted resale prices and the actual resale prices. This can be useful for further analysis or visualization.\n\n## Model Assessment: olsrr method\n\n### Generating tidy linear regression report\n\nWe use the olsrr package to generate a tidy report of the linear regression results, which provides a comprehensive overview of the model's performance and statistics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nolsrr::ols_regress(resale_mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                              Model Summary                                \n--------------------------------------------------------------------------\nR                           0.899       RMSE                    68982.259 \nR-Squared                   0.808       MSE                4773828300.418 \nAdj. R-Squared              0.807       Coef. Var                  12.344 \nPred R-Squared              0.806       AIC                    125639.432 \nMAE                     51860.197       SBC                    125750.224 \n--------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n AIC: Akaike Information Criteria \n SBC: Schwarz Bayesian Criteria \n\n                                     ANOVA                                      \n-------------------------------------------------------------------------------\n                    Sum of                                                     \n                   Squares          DF       Mean Square       F          Sig. \n-------------------------------------------------------------------------------\nRegression    1.000766e+14          15      6.671772e+12    1397.573    0.0000 \nResidual      2.379276e+13        4984    4773828300.418                       \nTotal         1.238693e+14        4999                                         \n-------------------------------------------------------------------------------\n\n                                                 Parameter Estimates                                                  \n---------------------------------------------------------------------------------------------------------------------\n                   model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n---------------------------------------------------------------------------------------------------------------------\n             (Intercept)    -173309.721      8916.656                 -19.437    0.000    -190790.291    -155829.152 \n          floor_area_sqm       5254.162        54.899        0.631     95.706    0.000       5146.536       5361.788 \n             floor_level     112895.436      3891.341        0.194     29.012    0.000     105266.695     120524.177 \n      remaining_lease_yr       5169.963        73.728        0.500     70.122    0.000       5025.423       5314.503 \n                PROX_CBD        -15.110         0.318       -0.417    -47.504    0.000        -15.734        -14.487 \n        PROX_ELDERLYCARE         -8.643         1.739       -0.034     -4.970    0.000        -12.053         -5.234 \n             PROX_HAWKER        -28.488         2.171       -0.094    -13.122    0.000        -32.744        -24.231 \n                PROX_MRT        -23.829         2.825       -0.058     -8.435    0.000        -29.367        -18.291 \n               PROX_PARK        -11.405         2.653       -0.031     -4.299    0.000        -16.606         -6.204 \n        PROX_GOOD_PRISCH         -1.705         0.843       -0.014     -2.022    0.043         -3.358         -0.052 \n               PROX_MALL         -5.746         3.010       -0.014     -1.909    0.056        -11.646          0.155 \n        PROX_SUPERMARKET          4.027         6.404        0.004      0.629    0.530         -8.529         16.582 \nWITHIN_350M_KINDERGARTEN       8187.352      1105.089        0.053      7.409    0.000       6020.892      10353.813 \n   WITHIN_350M_CHILDCARE      -2171.257       528.929       -0.031     -4.105    0.000      -3208.190      -1134.323 \n         WITHIN_350M_BUS         33.593       364.145        0.001      0.092    0.927       -680.291        747.477 \n       WITHIN_1KM_SCHOOL      -2084.913       775.082       -0.020     -2.690    0.007      -3604.414       -565.411 \n---------------------------------------------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\n## Multicolinarity\n\nWe assess multicollinearity among the predictors using the Variance Inflation Factor (VIF). High VIF values indicate multicollinearity, which can affect the stability of the regression coefficients. We display the VIF results in a table and create a plot to visualize the VIF values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols_vif_tol(resale_mlr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                  Variables Tolerance      VIF\n1            floor_area_sqm 0.8876293 1.126596\n2               floor_level 0.8646552 1.156530\n3        remaining_lease_yr 0.7590403 1.317453\n4                  PROX_CBD 0.4997118 2.001154\n5          PROX_ELDERLYCARE 0.8091410 1.235879\n6               PROX_HAWKER 0.7461858 1.340149\n7                  PROX_MRT 0.8229885 1.215084\n8                 PROX_PARK 0.7200841 1.388727\n9          PROX_GOOD_PRISCH 0.7685066 1.301225\n10                PROX_MALL 0.7558033 1.323096\n11         PROX_SUPERMARKET 0.8592631 1.163788\n12 WITHIN_350M_KINDERGARTEN 0.7568912 1.321194\n13    WITHIN_350M_CHILDCARE 0.6601779 1.514743\n14          WITHIN_350M_BUS 0.8621027 1.159955\n15        WITHIN_1KM_SCHOOL 0.6742910 1.483039\n```\n\n\n:::\n\n```{.r .cell-code}\nvif <- performance::check_collinearity(resale_mlr)\nkable(vif, \n      caption = \"Variance Inflation Factor (VIF) Results\") %>%\n  kable_styling(font_size = 18) \n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"font-size: 18px; margin-left: auto; margin-right: auto;\">\n<caption style=\"font-size: initial !important;\">Variance Inflation Factor (VIF) Results</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Term </th>\n   <th style=\"text-align:right;\"> VIF </th>\n   <th style=\"text-align:right;\"> VIF_CI_low </th>\n   <th style=\"text-align:right;\"> VIF_CI_high </th>\n   <th style=\"text-align:right;\"> SE_factor </th>\n   <th style=\"text-align:right;\"> Tolerance </th>\n   <th style=\"text-align:right;\"> Tolerance_CI_low </th>\n   <th style=\"text-align:right;\"> Tolerance_CI_high </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> floor_area_sqm </td>\n   <td style=\"text-align:right;\"> 1.126597 </td>\n   <td style=\"text-align:right;\"> 1.096316 </td>\n   <td style=\"text-align:right;\"> 1.166397 </td>\n   <td style=\"text-align:right;\"> 1.061413 </td>\n   <td style=\"text-align:right;\"> 0.8876293 </td>\n   <td style=\"text-align:right;\"> 0.8573413 </td>\n   <td style=\"text-align:right;\"> 0.9121457 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> floor_level </td>\n   <td style=\"text-align:right;\"> 1.156530 </td>\n   <td style=\"text-align:right;\"> 1.124163 </td>\n   <td style=\"text-align:right;\"> 1.197335 </td>\n   <td style=\"text-align:right;\"> 1.075421 </td>\n   <td style=\"text-align:right;\"> 0.8646552 </td>\n   <td style=\"text-align:right;\"> 0.8351880 </td>\n   <td style=\"text-align:right;\"> 0.8895507 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> remaining_lease_yr </td>\n   <td style=\"text-align:right;\"> 1.317453 </td>\n   <td style=\"text-align:right;\"> 1.275368 </td>\n   <td style=\"text-align:right;\"> 1.365970 </td>\n   <td style=\"text-align:right;\"> 1.147804 </td>\n   <td style=\"text-align:right;\"> 0.7590403 </td>\n   <td style=\"text-align:right;\"> 0.7320803 </td>\n   <td style=\"text-align:right;\"> 0.7840874 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> PROX_CBD </td>\n   <td style=\"text-align:right;\"> 2.001154 </td>\n   <td style=\"text-align:right;\"> 1.921584 </td>\n   <td style=\"text-align:right;\"> 2.087594 </td>\n   <td style=\"text-align:right;\"> 1.414621 </td>\n   <td style=\"text-align:right;\"> 0.4997118 </td>\n   <td style=\"text-align:right;\"> 0.4790204 </td>\n   <td style=\"text-align:right;\"> 0.5204041 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> PROX_ELDERLYCARE </td>\n   <td style=\"text-align:right;\"> 1.235879 </td>\n   <td style=\"text-align:right;\"> 1.198544 </td>\n   <td style=\"text-align:right;\"> 1.280234 </td>\n   <td style=\"text-align:right;\"> 1.111701 </td>\n   <td style=\"text-align:right;\"> 0.8091410 </td>\n   <td style=\"text-align:right;\"> 0.7811074 </td>\n   <td style=\"text-align:right;\"> 0.8343457 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> PROX_HAWKER </td>\n   <td style=\"text-align:right;\"> 1.340149 </td>\n   <td style=\"text-align:right;\"> 1.296772 </td>\n   <td style=\"text-align:right;\"> 1.389865 </td>\n   <td style=\"text-align:right;\"> 1.157648 </td>\n   <td style=\"text-align:right;\"> 0.7461858 </td>\n   <td style=\"text-align:right;\"> 0.7194943 </td>\n   <td style=\"text-align:right;\"> 0.7711454 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> PROX_MRT </td>\n   <td style=\"text-align:right;\"> 1.215084 </td>\n   <td style=\"text-align:right;\"> 1.179003 </td>\n   <td style=\"text-align:right;\"> 1.258437 </td>\n   <td style=\"text-align:right;\"> 1.102308 </td>\n   <td style=\"text-align:right;\"> 0.8229885 </td>\n   <td style=\"text-align:right;\"> 0.7946363 </td>\n   <td style=\"text-align:right;\"> 0.8481743 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> PROX_PARK </td>\n   <td style=\"text-align:right;\"> 1.388727 </td>\n   <td style=\"text-align:right;\"> 1.342614 </td>\n   <td style=\"text-align:right;\"> 1.441046 </td>\n   <td style=\"text-align:right;\"> 1.178442 </td>\n   <td style=\"text-align:right;\"> 0.7200841 </td>\n   <td style=\"text-align:right;\"> 0.6939405 </td>\n   <td style=\"text-align:right;\"> 0.7448158 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> PROX_GOOD_PRISCH </td>\n   <td style=\"text-align:right;\"> 1.301225 </td>\n   <td style=\"text-align:right;\"> 1.260069 </td>\n   <td style=\"text-align:right;\"> 1.348893 </td>\n   <td style=\"text-align:right;\"> 1.140713 </td>\n   <td style=\"text-align:right;\"> 0.7685066 </td>\n   <td style=\"text-align:right;\"> 0.7413485 </td>\n   <td style=\"text-align:right;\"> 0.7936071 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> PROX_MALL </td>\n   <td style=\"text-align:right;\"> 1.323095 </td>\n   <td style=\"text-align:right;\"> 1.280688 </td>\n   <td style=\"text-align:right;\"> 1.371910 </td>\n   <td style=\"text-align:right;\"> 1.150259 </td>\n   <td style=\"text-align:right;\"> 0.7558033 </td>\n   <td style=\"text-align:right;\"> 0.7289110 </td>\n   <td style=\"text-align:right;\"> 0.7808301 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> PROX_SUPERMARKET </td>\n   <td style=\"text-align:right;\"> 1.163788 </td>\n   <td style=\"text-align:right;\"> 1.130939 </td>\n   <td style=\"text-align:right;\"> 1.204877 </td>\n   <td style=\"text-align:right;\"> 1.078790 </td>\n   <td style=\"text-align:right;\"> 0.8592631 </td>\n   <td style=\"text-align:right;\"> 0.8299604 </td>\n   <td style=\"text-align:right;\"> 0.8842206 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> WITHIN_350M_KINDERGARTEN </td>\n   <td style=\"text-align:right;\"> 1.321194 </td>\n   <td style=\"text-align:right;\"> 1.278895 </td>\n   <td style=\"text-align:right;\"> 1.369908 </td>\n   <td style=\"text-align:right;\"> 1.149432 </td>\n   <td style=\"text-align:right;\"> 0.7568912 </td>\n   <td style=\"text-align:right;\"> 0.7299761 </td>\n   <td style=\"text-align:right;\"> 0.7819249 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> WITHIN_350M_CHILDCARE </td>\n   <td style=\"text-align:right;\"> 1.514743 </td>\n   <td style=\"text-align:right;\"> 1.461641 </td>\n   <td style=\"text-align:right;\"> 1.573954 </td>\n   <td style=\"text-align:right;\"> 1.230749 </td>\n   <td style=\"text-align:right;\"> 0.6601779 </td>\n   <td style=\"text-align:right;\"> 0.6353427 </td>\n   <td style=\"text-align:right;\"> 0.6841624 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> WITHIN_350M_BUS </td>\n   <td style=\"text-align:right;\"> 1.159955 </td>\n   <td style=\"text-align:right;\"> 1.127359 </td>\n   <td style=\"text-align:right;\"> 1.200892 </td>\n   <td style=\"text-align:right;\"> 1.077012 </td>\n   <td style=\"text-align:right;\"> 0.8621027 </td>\n   <td style=\"text-align:right;\"> 0.8327143 </td>\n   <td style=\"text-align:right;\"> 0.8870286 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> WITHIN_1KM_SCHOOL </td>\n   <td style=\"text-align:right;\"> 1.483039 </td>\n   <td style=\"text-align:right;\"> 1.431685 </td>\n   <td style=\"text-align:right;\"> 1.540503 </td>\n   <td style=\"text-align:right;\"> 1.217801 </td>\n   <td style=\"text-align:right;\"> 0.6742910 </td>\n   <td style=\"text-align:right;\"> 0.6491388 </td>\n   <td style=\"text-align:right;\"> 0.6984776 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n```{.r .cell-code}\nplot(vif) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nVariable `Component` is not in your data frame :/\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Take-Home_Exercise03_files/figure-html/unnamed-chunk-64-1.png){width=672}\n:::\n:::\n\n\n## Variable Selection\n\nIn this part, we perform variable selection using forward selection, which adds predictors to the model one at a time based on their statistical significance. We set a p-value threshold of 0.05 to determine which variables to include in the final model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresale_fw_mlr <- ols_step_forward_p(\n  resale_mlr,\n  p_val = 0.05,\n  details = FALSE)\n```\n:::\n\n\nWe visualize the results of the forward selection process, which helps us understand which variables were selected and their contributions to the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(resale_fw_mlr)\n```\n\n::: {.cell-output-display}\n![](Take-Home_Exercise03_files/figure-html/unnamed-chunk-66-1.png){width=1920}\n:::\n:::\n\n\n\n## Visualising model parameters\n\nFinally, we use the ggcoefstats() function to create a plot of the model coefficients, sorted in ascending order. This visualization allows us to easily interpret the effect sizes of the predictors in the MLR model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggcoefstats(resale_mlr,\n            sort = \"ascending\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNumber of labels is greater than default palette color count.\n• Select another color `palette` (and/or `package`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Take-Home_Exercise03_files/figure-html/unnamed-chunk-67-1.png){width=1152}\n:::\n:::\n\n\n## Test for Non-Linearity\n\nIn multiple linear regression, it is important for us to test the assumption that linearity and additivity of the relationship between dependent and independent variables.\n\nIn the code chunk below, the ols_plot_resid_fit() of olsrr package is used to perform linearity assumption test.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols_plot_resid_fit(resale_fw_mlr$model)\n```\n\n::: {.cell-output-display}\n![](Take-Home_Exercise03_files/figure-html/unnamed-chunk-68-1.png){width=672}\n:::\n:::\n\n\nThe figure above reveals that most of the data poitns are scattered around the 0 line, hence we can safely conclude that the relationships between the dependent variable and independent variables are linear.\n\n## Test for Normality Assumption\n\nLastly, the code chunk below uses ols_plot_resid_hist() of olsrr package to perform normality assumption test.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols_plot_resid_hist(resale_fw_mlr$model)\n```\n\n::: {.cell-output-display}\n![](Take-Home_Exercise03_files/figure-html/unnamed-chunk-69-1.png){width=672}\n:::\n:::\n\n\nThe figure reveals that the residual of the multiple linear regression model (i.e. condo.mlr1) is resemble normal distribution.\n\nIf you prefer formal statistical test methods, the ols_test_normality() of olsrr package can be used as shown in the code chun below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nols_test_normality(resale_fw_mlr$model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n-----------------------------------------------\n       Test             Statistic       pvalue  \n-----------------------------------------------\nShapiro-Wilk              0.964          0.0000 \nKolmogorov-Smirnov        0.048          0.0000 \nCramer-von Mises         421.9155        0.0000 \nAnderson-Darling         24.1198         0.0000 \n-----------------------------------------------\n```\n\n\n:::\n:::\n\n\nThe summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.\n\n## Testing for Spatial Autocorrelation\n\nThe hedonic model we try to build are using geographically referenced attributes, hence it is also important for us to visual the residual of the hedonic pricing model.\n\nFirst, we will export the residual of the hedonic pricing model and save it as a data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmlr_output <- as.data.frame(resale_fw_mlr$model$residuals) %>%\n  rename(`FW_MLR_RES` = `resale_fw_mlr$model$residuals`)\n```\n:::\n\n\nNext, we will join the newly created data frame with condo_resale_sf object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncondo_resale_sf <- cbind(train_data, \n                        mlr_output$FW_MLR_RES) %>%\n  rename(`MLR_RES` = `mlr_output.FW_MLR_RES`)\n```\n:::\n\n\nNext, we will use tmap package to display the distribution of the residuals on an interactive map.\n\nThe code churn below will turn on the interactive mode of tmap.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\ntmap mode set to plotting\n```\n\n\n:::\n\n```{.r .cell-code}\ntm_shape(mpsz)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale_sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The shape mpsz is invalid. See sf::st_is_valid\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nVariable(s) \"MLR_RES\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Take-Home_Exercise03_files/figure-html/unnamed-chunk-73-1.png){width=672}\n:::\n:::\n\n\nThe figure above reveal that there is sign of spatial autocorrelation.\n\n## Spatial stationary test\n\nTo proof that our observation is indeed true, the Moran’s I test will be performed\n\nHo: The residuals are randomly distributed (also known as spatial stationary)\n\nH1: The residuals are spatially non-stationary\n\nFirst, we will compute the distance-based weight matrix by using dnearneigh() function of spdep.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncondo_resale_sf <- condo_resale_sf %>%\n  mutate(nb = st_knn(geometry, k=6,\n                     longlat = FALSE),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1)\n```\n:::\n\n\nNext, global_moran_perm() of sfdep is used to perform global Moran permutation test.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglobal_moran_perm(condo_resale_sf$MLR_RES, \n                  condo_resale_sf$nb, \n                  condo_resale_sf$wt, \n                  alternative = \"two.sided\", \n                  nsim = 99)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMonte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.54398, observed rank = 100, p-value < 2.2e-16\nalternative hypothesis: two.sided\n```\n\n\n:::\n:::\n\n\nThe Global Moran’s I test for residual spatial autocorrelation shows that it’s p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.\n\nSince the Observed Global Moran I = 0.54398 which is greater than 0, we can infer than the residuals resemble cluster distribution.\n\n# Calibrating Conventional Random Forest Model\n\nFirst, we load the training dataset from an RDS file using the read_rds() function. This dataset will be used to train our Random Forest model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_data <- read_rds(\"data/rds/train_data.rds\")\n```\n:::\n\n\nHere, we remove any geometric information from the training dataset using the st_drop_geometry() function. This is important because the Random Forest model does not require spatial data for its predictions, and we want to focus solely on the relevant predictor variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_data_nogeom <- train_data %>%\n  st_drop_geometry()\n```\n:::\n\n\nIn this section, we fit a Random Forest model using the ranger() function. We specify the formula to predict resale_price based on various predictors, including floor_area_sqm, floor_level, and proximity to various amenities. We set a random seed for reproducibility, ensuring that the results can be consistently replicated.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#|eval: false \nset.seed(1234)\nrf <- ranger(resale_price ~ floor_area_sqm + floor_level + remaining_lease_yr + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +PROX_MRT + PROX_PARK + PROX_GOOD_PRISCH + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_SCHOOL,\n             data=train_data_nogeom)\n```\n:::\n\n\nHere, we save the fitted Random Forest model to an RDS file using the write_rds() function. This allows us to easily load the model later without needing to refit it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(rf, \"data/rds/rf.rds\")\n```\n:::\n\n\nIn this step, we load the previously saved Random Forest model from the RDS file using the read_rds() function. We can then print the model object to confirm that it has been loaded successfully.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf <- read_rds(\"data/rds/rf.rds\")\nrf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRanger result\n\nCall:\n ranger(resale_price ~ floor_area_sqm + floor_level + remaining_lease_yr +      PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + PROX_MRT + PROX_PARK +      PROX_GOOD_PRISCH + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +      WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_SCHOOL,      data = train_data_nogeom) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      5000 \nNumber of independent variables:  15 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       2296133442 \nR squared (OOB):                  0.9073348 \n```\n\n\n:::\n:::\n\n\nWe apply the same process to the test dataset, removing any geometric information using st_drop_geometry(). This prepares the test data for making predictions with the Random Forest model. We use the fitted Random Forest model to make predictions on the test dataset by calling the predict() function. The predictions are extracted from the resulting object. We calculate the Root Mean Square Error (RMSE) to assess the model's prediction accuracy. We first extract the actual resale prices from the test data, then compute the RMSE by taking the square root of the average of the squared differences between the predicted and actual values. Finally, we print the RMSE value. We calculate the R-squared value to evaluate the proportion of variance in the resale prices explained by the model. We use the formula for R-squared, which compares the sum of squared errors from our predictions to the total sum of squares. We then print the R-squared value. Finally, We create a new data frame that combines the predicted resale prices and the actual resale prices. This can be useful for further analysis or visualization\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_data_nogeom <- test_data %>%\n  st_drop_geometry()\n\n# Make predictions on the test data\npredictions_rf <- predict(rf, data=test_data_nogeom)$predictions\n\n# Calculate RMSE\nactuals_rf <- test_data$resale_price\nrmse_rf <- sqrt(mean((predictions_rf - actuals_rf)^2))\n\n# Print RMSE\nprint(paste(\"Root Mean Square Error:\", rmse_rf))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Root Mean Square Error: 79118.0864819995\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate R-squared\nactuals_rf <- test_data$resale_price\nr_squared_rf <- 1 - (sum((predictions_rf - actuals_rf)^2) / sum((actuals_rf - mean(actuals_rf))^2))\n\n# Print R-squared\nprint(paste(\"R-squared for Random Forest:\", r_squared_rf))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"R-squared for Random Forest: 0.800619218948169\"\n```\n\n\n:::\n\n```{.r .cell-code}\ncombined_df_rf <- data.frame(predictions_rf,\n                           actuals_rf)\n```\n:::\n\n\n# Geographically Weighted Regrssion Model\n\nIn this section, we will calibrate a model to predict the resale price by using geographically weighted regression methods of the GWmodel package.\n\n## Converting the sf data.frame to SpatialPointDataFrame\n\nFirst, we convert the sf (simple features) data frame train_data into a SpatialPointDataFrame using the as_Spatial() function. This conversion is necessary because the Geographically Weighted Regression (GWR) methods in the GWmodel package require spatial data in this format.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_data_sp <- as_Spatial(train_data)\ntrain_data_sp\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclass       : SpatialPointsDataFrame \nfeatures    : 5000 \nextent      : 11806.62, 45192.04, 28097.21, 48682.57  (xmin, xmax, ymin, ymax)\ncrs         : +proj=tmerc +lat_0=1.36666666666667 +lon_0=103.833333333333 +k=1 +x_0=28001.642 +y_0=38744.572 +ellps=WGS84 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nvariables   : 25\nnames       : resale_price, floor_area_sqm, floor_level, remaining_lease_yr,         PROX_CBD,  PROX_ELDERLYCARE,      PROX_HAWKER,         PROX_MRT,        PROX_PARK, PROX_GOOD_PRISCH,         PROX_MALL,   PROX_SUPERMARKET, WITHIN_350M_KINDERGARTEN, WITHIN_350M_CHILDCARE, WITHIN_350M_BUS, ... \nmin values  :       255000,             52,           1,                 42, 666.644171819409, 0.179647493572076, 6.79525704960773, 43.4480854308872, 69.1086103057036, 49.5189588300732, 0.408661136877515, 0.0574108534731242,                        0,                     0,               0, ... \nmax values  :      1450000,            153,           3,                 95, 20122.7129321506,  4767.18822737805, 2830.83991796497, 3453.82285464761, 2412.04585964894, 7508.35652041974,  3158.74708636409,     3325.044142153,                        8,                    18,              18, ... \n```\n\n\n:::\n:::\n\n\n## Computing adaptive bandwidth\n\nHere, we compute the optimal bandwidth for the GWR model using the bw.gwr() function from the GWmodel package. We specify the formula to predict resale_price based on various predictors, and we set the approach to \"CV\" (cross-validation) to determine the best bandwidth. We also choose a Gaussian kernel and indicate that we want an adaptive bandwidth.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbw_adaptive <- bw.gwr(resale_price ~ floor_area_sqm + floor_level + remaining_lease_yr + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +PROX_MRT + PROX_PARK + PROX_GOOD_PRISCH + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_SCHOOL,\n                  data=train_data_sp,\n                  approach=\"CV\",\n                  kernel=\"gaussian\",\n                  adaptive=TRUE,\n                  longlat=FALSE)\n```\n:::\n\n\nIn this step, we save the computed adaptive bandwidth object to an RDS file using the write_rds() function. This allows us to easily load the bandwidth later without needing to recompute it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(bw_adaptive, \"data/rds/bw_adaptive.rds\")\n```\n:::\n\n\nWe load the previously saved adaptive bandwidth object from the RDS file using the read_rds() function. This enables us to use the bandwidth in the subsequent GWR model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbw_adaptive <- read_rds(\"data/rds/bw_adaptive.rds\")\n```\n:::\n\n\nHere, we print the adaptive bandwidth object to the console to review its contents and confirm that it has been loaded correctly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbw_adaptive\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 124\n```\n\n\n:::\n:::\n\n\n## Constructing the adaptive bandwidth gwr model\n\nIn this section, we fit the GWR model using the gwr.basic() function. We specify the same formula for predicting resale_price and use the adaptive bandwidth we computed earlier. We also choose a Gaussian kernel for the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngwr_adaptive <- gwr.basic(formula = resale_price ~ floor_area_sqm + floor_level + remaining_lease_yr + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +PROX_MRT + PROX_PARK + PROX_GOOD_PRISCH + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_SCHOOL,\n                          data=train_data_sp,\n                          bw=bw_adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE,\n                          longlat = FALSE)\n```\n:::\n\n\nHere, we save the fitted GWR model to an RDS file using the write_rds() function. This allows us to easily load the model later for analysis or visualization.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(gwr_adaptive, \"data/rds/gwr_adaptive.rds\")\n```\n:::\n\n\n## Retrieve gwr output object\n\nWe load the previously saved GWR model from the RDS file using the read_rds() function. This enables us to access the model output for further analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngwr_adaptive <- read_rds(\"data/rds/gwr_adaptive.rds\")\n```\n:::\n\n\nIn this step, we print the GWR model output to the console to review the results and coefficients of the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngwr_adaptive\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2024-11-08 17:50:52.092242 \n   Call:\n   gwr.basic(formula = resale_price ~ floor_area_sqm + floor_level + \n    remaining_lease_yr + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER + \n    PROX_MRT + PROX_PARK + PROX_GOOD_PRISCH + PROX_MALL + PROX_SUPERMARKET + \n    WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + \n    WITHIN_1KM_SCHOOL, data = train_data_sp, bw = bw_adaptive, \n    kernel = \"gaussian\", adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  resale_price\n   Independent variables:  floor_area_sqm floor_level remaining_lease_yr PROX_CBD PROX_ELDERLYCARE PROX_HAWKER PROX_MRT PROX_PARK PROX_GOOD_PRISCH PROX_MALL PROX_SUPERMARKET WITHIN_350M_KINDERGARTEN WITHIN_350M_CHILDCARE WITHIN_350M_BUS WITHIN_1KM_SCHOOL\n   Number of data points: 5000\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n    Min      1Q  Median      3Q     Max \n-248982  -43544   -4735   37058  459727 \n\n   Coefficients:\n                              Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)              -1.733e+05  8.917e+03 -19.437  < 2e-16 ***\n   floor_area_sqm            5.254e+03  5.490e+01  95.706  < 2e-16 ***\n   floor_level               1.129e+05  3.891e+03  29.012  < 2e-16 ***\n   remaining_lease_yr        5.170e+03  7.373e+01  70.122  < 2e-16 ***\n   PROX_CBD                 -1.511e+01  3.181e-01 -47.504  < 2e-16 ***\n   PROX_ELDERLYCARE         -8.643e+00  1.739e+00  -4.970 6.91e-07 ***\n   PROX_HAWKER              -2.849e+01  2.171e+00 -13.122  < 2e-16 ***\n   PROX_MRT                 -2.383e+01  2.825e+00  -8.435  < 2e-16 ***\n   PROX_PARK                -1.140e+01  2.653e+00  -4.299 1.75e-05 ***\n   PROX_GOOD_PRISCH         -1.705e+00  8.433e-01  -2.022  0.04326 *  \n   PROX_MALL                -5.746e+00  3.010e+00  -1.909  0.05631 .  \n   PROX_SUPERMARKET          4.027e+00  6.404e+00   0.629  0.52954    \n   WITHIN_350M_KINDERGARTEN  8.187e+03  1.105e+03   7.409 1.49e-13 ***\n   WITHIN_350M_CHILDCARE    -2.171e+03  5.289e+02  -4.105 4.11e-05 ***\n   WITHIN_350M_BUS           3.359e+01  3.641e+02   0.092  0.92650    \n   WITHIN_1KM_SCHOOL        -2.085e+03  7.751e+02  -2.690  0.00717 ** \n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 69090 on 4984 degrees of freedom\n   Multiple R-squared: 0.8079\n   Adjusted R-squared: 0.8073 \n   F-statistic:  1398 on 15 and 4984 DF,  p-value: < 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 2.379276e+13\n   Sigma(hat): 68996.06\n   AIC:  125639.4\n   AICc:  125639.6\n   BIC:  120895\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 124 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                                   Min.     1st Qu.      Median     3rd Qu.\n   Intercept                -1.1883e+06 -3.2380e+05 -1.5201e+05 -6.2623e+04\n   floor_area_sqm            3.4660e+03  4.5846e+03  5.1445e+03  5.9253e+03\n   floor_level              -4.0657e+04  3.2044e+04  6.3526e+04  8.9361e+04\n   remaining_lease_yr        3.2847e+03  4.4774e+03  6.3236e+03  7.0195e+03\n   PROX_CBD                 -5.4535e+01 -2.2195e+01 -1.3516e+01 -2.7769e+00\n   PROX_ELDERLYCARE         -9.2159e+01 -2.2286e+01 -8.8840e+00  2.8320e+00\n   PROX_HAWKER              -9.7555e+01 -3.2985e+01 -1.9348e+01 -1.4133e+00\n   PROX_MRT                 -1.2126e+02 -6.0658e+01 -4.0671e+01 -2.3089e+01\n   PROX_PARK                -1.0389e+02 -2.2471e+01 -7.9911e+00  9.3400e+00\n   PROX_GOOD_PRISCH         -9.2503e+01 -2.0325e+01 -4.5114e+00  3.4717e+00\n   PROX_MALL                -1.3243e+02 -2.4448e+01 -2.2021e+00  1.6779e+01\n   PROX_SUPERMARKET         -1.5516e+02 -3.6928e+01 -1.4143e+01  1.3301e+01\n   WITHIN_350M_KINDERGARTEN -2.7037e+04 -4.3264e+03 -4.9297e+02  4.5013e+03\n   WITHIN_350M_CHILDCARE    -7.7435e+03 -7.7061e+02  6.8761e+02  1.9763e+03\n   WITHIN_350M_BUS          -3.5752e+03 -5.1185e+01  9.3638e+02  1.9073e+03\n   WITHIN_1KM_SCHOOL        -2.8814e+04 -4.3702e+03  1.3955e+03  6.0671e+03\n                                  Max.\n   Intercept                254923.658\n   floor_area_sqm             7667.623\n   floor_level              156689.974\n   remaining_lease_yr         9726.877\n   PROX_CBD                     44.247\n   PROX_ELDERLYCARE             85.661\n   PROX_HAWKER                  54.797\n   PROX_MRT                     35.603\n   PROX_PARK                    95.599\n   PROX_GOOD_PRISCH             56.057\n   PROX_MALL                    76.656\n   PROX_SUPERMARKET             66.338\n   WITHIN_350M_KINDERGARTEN  18213.354\n   WITHIN_350M_CHILDCARE      8068.748\n   WITHIN_350M_BUS            5054.024\n   WITHIN_1KM_SCHOOL         20116.156\n   ************************Diagnostic information*************************\n   Number of data points: 5000 \n   Effective number of parameters (2trace(S) - trace(S'S)): 369.0056 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 4630.994 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 121724.4 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 121398.2 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 118566.5 \n   Residual sum of squares: 9.68184e+12 \n   R-square value:  0.9218383 \n   Adjusted R-square value:  0.9156089 \n\n   ***********************************************************************\n   Program stops at: 2024-11-08 17:51:21.713201 \n```\n\n\n:::\n:::\n\n\n### Converting SDF into sf data.frame\n\nWe convert the spatial data frame (SDF) from the GWR model output into a regular data frame using as.data.frame(). We also select specific columns to keep, excluding columns 2 to 15. Then, we combine the original training data with the GWR output to create a new spatial data frame gwr_sf_adaptive.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngwr_adaptive_output <- as.data.frame(\n  gwr_adaptive$SDF) %>%\n  select(-c(2:15))\n\ngwr_sf_adaptive <- cbind(train_data_sp,\n                         gwr_adaptive_output)\n```\n:::\n\n\nHere, we summarize the predicted values (yhat) from the GWR model output. This provides an overview of the predicted resale prices across the spatial data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(gwr_adaptive$SDF$yhat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 208888  457845  554428  558823  638219 1302016 \n```\n\n\n:::\n:::\n\n\n### Visualising local R2\n\nIn this section, we create an interactive point symbol map to visualize the local R-squared values from the GWR model. We use the tmap package to plot the base map (mpsz) with a specified transparency and overlay the points from gwr_sf_adaptive, colored by the local R-squared values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\ntmap mode set to plotting\n```\n\n\n:::\n\n```{.r .cell-code}\ntmap_options(check.and.fix = TRUE)\ntm_shape(mpsz)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(gwr_sf_adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The shape mpsz is invalid. See sf::st_is_valid\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Take-Home_Exercise03_files/figure-html/unnamed-chunk-93-1.png){width=672}\n:::\n:::\n\n\n### Visualising coefficient estimates\n\nWe create two separate interactive point symbol maps to visualize the coefficient estimates for floor_area_sqm_SE and floor_area_sqm_TV. We arrange these maps side by side for comparison, ensuring they share the same aspect ratio and zoom level.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_options(check.and.fix = TRUE)\ntmap_mode(\"plot\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\ntmap mode set to plotting\n```\n\n\n:::\n\n```{.r .cell-code}\nAREA_SQM_SE <- tm_shape(mpsz)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(gwr_sf_adaptive) +  \n  tm_dots(col = \"floor_area_sqm_SE\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\nAREA_SQM_TV <- tm_shape(mpsz)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(gwr_sf_adaptive) +  \n  tm_dots(col = \"floor_area_sqm_TV\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The shape mpsz is invalid. See sf::st_is_valid\nWarning: The shape mpsz is invalid. See sf::st_is_valid\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Take-Home_Exercise03_files/figure-html/unnamed-chunk-94-1.png){width=672}\n:::\n:::\n\n\nFinally, we create a bubble map to visualize the local R-squared values specifically for the \"CENTRAL REGION\". The bubbles represent the local R-squared values, with their size indicating the magnitude of the values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntm_shape(mpsz[mpsz$REGION_N==\"CENTRAL REGION\", ])+\n  tm_polygons()+\ntm_shape(gwr_sf_adaptive) + \n  tm_bubbles(col = \"Local_R2\",\n           size = 0.15,\n           border.col = \"gray60\",\n           border.lwd = 1)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The shape mpsz[mpsz$REGION_N == \"CENTRAL REGION\", ] is invalid. See\nsf::st_is_valid\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Take-Home_Exercise03_files/figure-html/unnamed-chunk-95-1.png){width=672}\n:::\n:::\n\n\n# Calibrating Geographical Random Forest Model\n\n## Preparing coordinates data\n\n### Extracting coordinates data\n\nIn this section, we are extracting the spatial coordinates from our datasets. The **`st_coordinates()`** function is used to obtain the coordinates (latitude and longitude) from spatial objects. Here, we extract coordinates for three datasets:\n\n-   **`resale_tidy`**: This might be our complete dataset.\n\n-   **`train_data`**: The dataset we will use for training our model.\n\n-   **`test_data`**: The dataset we will use for testing our model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoords <- st_coordinates(resale_tidy)\ncoords_train <- st_coordinates(train_data)\ncoords_test <- st_coordinates(test_data)\n```\n:::\n\n\nIn this part, we save the extracted coordinates for both the training and testing datasets to RDS files. The **`write_rds()`** function is used to write R objects to a file in R's native format. The **`eval: false`** comment indicates that this code should not be executed in the current context, possibly to avoid overwriting existing files.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoords_train <- write_rds(coords_train, \"data/rds/coords_train.rds\" )\ncoords_test <- write_rds(coords_test, \"data/rds/coords_test.rds\" )\n```\n:::\n\n\nHere, we load the previously saved coordinates from the RDS files back into our R environment using the **`read_rds()`** function. This allows us to use the coordinates later in our analysis without needing to extract them again.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoords_train = read_rds(\"data/rds/coords_train.rds\")\ncoords_test <- read_rds(\"data/rds/coords_test.rds\")\n```\n:::\n\n\n## Droping geometry field\n\nIn this line, we load the training dataset from an RDS file.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_data <- read_rds(\"data/rds/train_data.rds\")\n```\n:::\n\n\nWe then drop the geometry field from the training dataset using the **`st_drop_geometry()`** function. This is necessary because the GRF model requires only the attribute data (predictors) and not the spatial geometry itself.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_data_nogeom <- train_data %>% \n  st_drop_geometry()\n```\n:::\n\n\n## Calibrating Geographically Random Forest Model\n\n**Geographically Weighted Random Forest (GRF)** is a spatial analysis method that combines the principles of Random Forest with the concept of Geographically Weighted Regression (GWR). Unlike traditional GWR, which assumes a linear relationship, GRF can model non-linear relationships and is less prone to overfitting due to its bootstrapping nature.\n\n### GRF bandwidth\n\nIn this section, we calculate the bandwidth for the GRF model using the **`grf.bw()`** function. The bandwidth determines how much local information is used when making predictions. We specify the formula for the model, the training data without geometry, the coordinates, the number of trees to use (50), and the kernel type (adaptive). The adaptive kernel allows the model to adjust the influence of nearby points based on their distance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrf_bw <- grf.bw(resale_price ~ floor_area_sqm + floor_level + remaining_lease_yr + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +PROX_MRT + PROX_PARK + PROX_GOOD_PRISCH + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_SCHOOL,\n                  data=train_data_nogeom,\n                  coords= coords_train,\n                  trees = 50  ,\n                  kernel=\"adaptive\"                 )\n```\n:::\n\n\nWith a bandwidth of 270 in the GRF model indicates that the model will consider observations within a 270-unit radius when making predictions.\n\nHere, we save the calculated bandwidth object to an RDS file for later use.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(grf_bw, \"data/rds/grf_bw.rds\")\n```\n:::\n\n\nWe load the saved bandwidth object back into our R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrf_bw <- read_rds(\"data/rds/grf_bw.rds\")\n```\n:::\n\n\n#### Calibrating using training data\n\nIn this final section, we calibrate the Geographically Weighted Random Forest model using the **`grf()`** function. We set a seed for reproducibility with **`set.seed(1234)`**, ensuring that our results can be replicated. The formula specifies the response variable (**`resale_price`**) and the predictor variables. We provide the training data without geometry, the previously calculated bandwidth (**`grf_bw`**), the kernel type (adaptive), the coordinates, and the number of trees to use (100). This model will now be trained to predict resale prices based on the specified features while accounting for spatial relationships.\n\nBy following these steps, we effectively prepare our data, calculate necessary parameters, and calibrate a Geographically Weighted Random Forest model to analyze the spatial dynamics of resale prices.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\ngwRF_adaptive <- grf(formula = resale_price ~ floor_area_sqm + floor_level + remaining_lease_yr + PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +PROX_MRT + PROX_PARK + PROX_GOOD_PRISCH + PROX_MALL + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN + WITHIN_350M_CHILDCARE + WITHIN_350M_BUS + WITHIN_1KM_SCHOOL,\n                     dframe=train_data_nogeom, \n                     bw=grf_bw,\n                     kernel=\"adaptive\",\n                     coords=coords_train,\n                  ntree = 100)\n```\n:::\n\n\n#### **1. Residuals OOB (Out-Of-Bag):**\n\n-   **Min., 1st Qu., Median, Mean, 3rd Qu., Max.**: These statistics describe the distribution of the residuals (the differences between the observed and predicted values) for the out-of-bag samples.\n\n    -   **Min.**: The minimum residual value is -319,000, indicating that some predictions were significantly lower than the actual values.\n\n    -   **1st Qu.**: The first quartile (25th percentile) is -28,225.3, meaning that 25% of the residuals are below this value.\n\n    -   **Median**: The median residual is -1,837.4, suggesting that half of the predictions are below this value and half are above.\n\n    -   **Mean**: The mean residual is 173.6, indicating that, on average, the model slightly overestimates the actual values.\n\n    -   **3rd Qu.**: The third quartile (75th percentile) is 25,668.4, meaning that 75% of the residuals are below this value.\n\n    -   **Max.**: The maximum residual is 510,000, indicating that some predictions were significantly higher than the actual values.\n\n#### **2. Residuals Predicted (Not OOB):**\n\n-   This section provides similar statistics for the residuals calculated from the predictions made on the entire dataset (not just the out-of-bag samples).\n\n    -   The values here are generally smaller in magnitude compared to the OOB residuals, indicating that the model fits the training data better than it does the out-of-bag samples.\n\n#### **3. Local Variable Importance:**\n\n-   This section would typically provide insights into which variables were most important in making predictions. However, it seems that specific details are not included in your output. Variable importance can help you understand which predictors have the most influence on the model's predictions.\n\n#### **4. Mean Squared Error (MSE):**\n\n-   **MSE (OOB)**: 3,370,726,309.413\n\n    -   This is the average of the squared differences between the predicted and actual values for the out-of-bag samples. A lower MSE indicates a better fit.\n\n-   **Mean Squared Error Predicted (Not OOB)**: 67,831,868.939\n\n    -   This is the MSE for the predictions made on the entire dataset. The significantly lower value compared to the OOB MSE suggests that the model fits the training data very well.\n\n#### **5. R-squared:**\n\n-   **R-squared (OOB)**: 86.394%\n\n    -   This indicates that approximately 86.4% of the variance in the out-of-bag data can be explained by the model. This is a relatively high value, suggesting a good fit.\n\n-   **R-squared Predicted (Not OOB)**: 99.726%\n\n    -   This very high R-squared value indicates that the model explains almost all the variance in the training data. However, such a high value can sometimes indicate overfitting, where the model performs well on the training data but may not generalize well to unseen data.\n\n#### **6. AIC and AICc:**\n\n-   **AIC (OOB)**: 109,723.97\n\n-   **AICc (OOB)**: 109,724.08\n\n-   **AIC Predicted (Not OOB)**: 90,194.713\n\n-   **AICc Predicted (Not OOB)**: 90,194.823\n\n    -   The Akaike Information Criterion (AIC) and its corrected version (AICc) are measures of model quality. Lower values indicate a better model fit when comparing multiple models. The AIC values for the out-of-bag samples are higher than those for the predictions on the entire dataset, which is expected.\n\nLet’s save the model output by using the code chunk below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(gwRF_adaptive, \"data/rds/gwRF_adaptive.rds\")\n```\n:::\n\n\nThe code chunk below can be used to retrieve the save model in future.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngwRF_adaptive <- read_rds(\"data/rds/gwRF_adaptive.rds\")\n```\n:::\n\n\n## Predicting by using test data\n\n### Preparing the test data\n\nIn this section, we prepare the test dataset for predictions. We use the cbind() function to combine the test data with the extracted coordinates (coords_test). After that, we drop the geometry field using st_drop_geometry(), as the GRF model requires only the attribute data (predictors) and not the spatial geometry itself. The resulting test_data_nogeom will contain the necessary features for making predictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_data_nogeom <- cbind(test_data, coords_test) %>%\n  st_drop_geometry()\n```\n:::\n\n\n### Predicting with test data\n\nNext, we use the predict.grf() function from the SpatialML package to predict resale values using the test data and the previously calibrated GRF model (gwRF_adaptive).\n\nIn this line, we call the **`predict.grf()`** function to generate predictions. We pass the following arguments:\n\n-   **`gwRF_adaptive`**: The calibrated GRF model we created earlier.\n\n-   **`test_data_nogeom`**: The test dataset without geometry.\n\n-   **`x.var.name`** and **`y.var.name`**: These specify the names of the variables representing the X and Y coordinates, respectively. This is important for the spatial aspect of the predictions.\n\n-   **`local.w=1`**: This indicates that we want to use local weights for the predictions, meaning that the model will consider nearby observations more heavily.\n\n-   **`global.w=0`**: This indicates that we do not want to use global weights, focusing solely on local influences.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngwRF_pred <- predict.grf(gwRF_adaptive, \n                           test_data_nogeom, \n                           x.var.name=\"X\",\n                           y.var.name=\"Y\", \n                           local.w=1,\n                           global.w=0)\n```\n:::\n\n\nHere, we save the predictions generated by the GRF model to an RDS file using the **`write_rds()`** function. The **`eval: false`** comment indicates that this code should not be executed in the current context, possibly to avoid overwriting existing files.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGRF_pred <- write_rds(gwRF_pred, \"data/rds/GRF_pred.rds\")\n```\n:::\n\n\n### Converting the predicting output into a data frame\n\nIn this section, we load the saved predictions from the RDS file back into our R environment. We then convert the predictions into a data frame using **`as.data.frame()`**, which allows us to work with the predictions more easily in subsequent steps.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGRF_pred <- read_rds(\"data/rds/GRF_pred.rds\")\nGRF_pred_df <- as.data.frame(GRF_pred)\n```\n:::\n\n\nHere, we use **`cbind()`** again to append the predicted values from **`GRF_pred_df`** onto the **`test_data_nogeom`**. The resulting **`test_data_p`** will contain both the original test data (without geometry) and the predicted resale prices.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_data_p <- cbind(test_data_nogeom, GRF_pred_df)\n```\n:::\n\n\nIn this line, we save the combined test dataset (which now includes the predicted values) to an RDS file for future use.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(test_data_p, \"data/rds/test_data_p.rds\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_data_p <- read_rds(\"data/rds/test_data_p.rds\")\n```\n:::\n\n\n## Calculating Mean Errors and R2\n\nIn this section, we calculate various error metrics to evaluate the performance of our GRF model:\n\n-   **Mean Absolute Error (MAE)**: This metric gives us the average absolute difference between the actual and predicted resale prices, providing a straightforward measure of prediction accuracy.\n\n-   **Mean Squared Error (MSE)**: This metric squares the differences between actual and predicted values, penalizing larger errors more heavily.\n\n-   **Root Mean Squared Error (RMSE)**: This is the square root of the MSE, bringing the error metric back to the original units of the resale prices, making it easier to interpret.\n\n-   **R-squared (R²)**: This statistic indicates the proportion of variance in the actual resale prices that can be explained by the model. A higher R² value suggests a better fit.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nactual_prices <- test_data_p$resale_price\npredicted_prices <- test_data_p$GRF_pred \n\n# Calculate Mean Absolute Error (MAE)\nmae <- mean(abs(actual_prices - predicted_prices))\n\n# Calculate Mean Squared Error (MSE)\nmse <- mean((actual_prices - predicted_prices)^2)\n\n# Calculate Root Mean Squared Error (RMSE)\nrmse_gwRF <- sqrt(mse)\n\n# Calculate R-squared (R²)\nss_total <- sum((actual_prices - mean(actual_prices))^2)\nss_residual <- sum((actual_prices - predicted_prices)^2)\nr_squared_gwRF <- 1 - (ss_residual / ss_total)\n\n# Print the results\ncat(\"Mean Absolute Error (MAE):\", mae, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMean Absolute Error (MAE): 59680.74 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Mean Squared Error (MSE):\", mse, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMean Squared Error (MSE): 6916580376 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Root Mean Squared Error (RMSE):\", rmse_gwRF, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRoot Mean Squared Error (RMSE): 83165.98 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"R-squared (R²):\", r_squared_gwRF, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nR-squared (R²): 0.7796956 \n```\n\n\n:::\n:::\n\n\n# Model Comparison\n\nIn this part, we create a data frame to compare the performance of different models (Multiple Linear Regression, Random Forest, and Geographically Weighted Random Forest) based on their RMSE and R² values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model Comparison\nmodel_results <- data.frame(\n  Model = c(\"MLR\", \"Random Forest\", \"GWRF\"),\n  RMSE = c(rmse_mlr, rmse_rf, rmse_gwRF),\n  R_squared = c(r_squared_mlr, r_squared_rf, r_squared_gwRF)\n)\n```\n:::\n\n\nWe use **`ggplot2`** to create a bar plot that visually compares the RMSE values of the different models. This helps us quickly assess which model performed best in terms of prediction accuracy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(model_results, aes(x = Model, y = RMSE)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Model Comparison: RMSE\", y = \"Root Mean Squared Error (RMSE)\")\n```\n\n::: {.cell-output-display}\n![](Take-Home_Exercise03_files/figure-html/unnamed-chunk-116-1.png){width=672}\n:::\n:::\n\n\nFor each model, we create scatter plots to visualize the relationship between predicted and actual resale prices.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Enhanced Visualization MLR\nggplot(data = combined_df_mlr, aes(x = predictions_mlr, y = actuals_mlr)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", color = \"blue\", se = FALSE) +\n  labs(title = \"Predicted vs Actual Resale Prices MLR\",\n       x = \"Predicted Price\",\n       y = \"Actual Price\") +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Take-Home_Exercise03_files/figure-html/unnamed-chunk-117-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Enhanced Visualization RF\nggplot(data = combined_df_rf, aes(x = predictions_rf, y = actuals_rf)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", color = \"blue\", se = FALSE) +\n  labs(title = \"Predicted vs Actual Resale Prices RF\",\n       x = \"Predicted Price\",\n       y = \"Actual Price\") +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Take-Home_Exercise03_files/figure-html/unnamed-chunk-117-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Enhanced Visualization GWRF\nggplot(data = test_data_p, aes(x = GRF_pred, y = resale_price)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", color = \"blue\", se = FALSE) +\n  labs(title = \"Predicted vs Actual Resale Prices GRF\",\n       x = \"Predicted Price\",\n       y = \"Actual Price\") +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](Take-Home_Exercise03_files/figure-html/unnamed-chunk-117-3.png){width=672}\n:::\n:::\n\n\nThese plots help us visually assess how well each model's predictions align with the actual resale prices. A better predictive model will show points clustered closely around the diagonal line, indicating accurate predictions.\n\n## **Model Performance Comparison:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          Model     RMSE R_squared\n1           MLR 93308.44 0.8079205\n2 Random Forest 79118.09 0.8006192\n3          GWRF 83165.98 0.7796956\n```\n\n\n:::\n:::\n\n\n###  **Multiple Linear Regression (MLR)**:\n\n    -   **RMSE**: 93,308.44\n\n    -   **R-squared**: 0.8079\n\n    The MLR model has the highest RMSE among the three models, which indicates that its predictions are, on average, further away from the actual values compared to the other models. However, it has a relatively high R-squared value of 0.8079, meaning that it explains about 80.79% of the variance in the data. This suggests that while the model captures a good amount of the variability, its predictions are not as accurate as those of the other models.\n\n###  **Random Forest**:\n\n    -   **RMSE**: 79,118.09\n\n    -   **R-squared**: 0.8006\n\n    The Random Forest model has the lowest RMSE of 79,118.09, indicating that it provides the most accurate predictions on average among the three models. Its R-squared value of 0.8006 shows that it explains about 80.06% of the variance in the data, which is slightly lower than that of the MLR model. However, the lower RMSE suggests that the Random Forest model is better at making precise predictions, even if it explains a slightly smaller proportion of the variance.\n\n###  **Geographically Weighted Random Forest (GWRF)**:\n\n    -   **RMSE**: 83,165.98\n\n    -   **R-squared**: 0.7797\n\n    The GWRF model has an RMSE of 83,165.98, which is higher than that of the Random Forest but lower than that of the MLR. Its R-squared value of 0.7797 indicates that it explains about 77.97% of the variance in the data, which is the lowest among the three models. This suggests that while GWRF incorporates geographical information, it does not perform as well in terms of prediction accuracy or variance explanation compared to the other models.\n\n## **Conclusion:**\n\nIn summary, the **Random Forest model** performs the best overall, as it has the lowest RMSE, indicating the most accurate predictions. Although its R-squared value is slightly lower than that of the MLR model, the trade-off for better prediction accuracy makes it the preferred choice. The **Multiple Linear Regression** model, while explaining a higher percentage of variance, has the highest RMSE, which means its predictions are less reliable. The **GWRF model**, despite its geographical considerations, does not outperform the other two models in terms of prediction accuracy or variance explanation.\n\nTherefore, if we prioritize prediction accuracy, the **Random Forest model** is the best option among the three.\n\nThere are several reasons why the Geographically Weighted Random Forest (GWRF) model might not be performing as well as the other models, such as Multiple Linear Regression (MLR) and Random Forest. Here are some key factors to consider:\n\n### **1. Model Complexity and Overfitting:**\n\nOne possibility is that the GWRF model may be overfitting the training data. Overfitting occurs when a model learns not only the underlying patterns in the data but also the noise. This can lead to excellent performance on the training set but poor generalization to new, unseen data. In the case of GWRF, if the model is too complex or if it has too many trees, it might capture local variations that do not represent the overall trend, resulting in less accurate predictions.\n\n### **2. Insufficient Number of Trees:**\n\nAnother reason could be related to the number of trees used in the GWRF model. If the model has too few trees, it may not be able to capture the complexity of the data adequately. Random Forest models generally benefit from having a larger number of trees, as this helps to average out predictions and reduce variance. If the GWRF model is using a lower number of trees compared to the Random Forest model, it might not be leveraging the full potential of the ensemble learning approach, leading to poorer performance.\n\n### **3. Geographical Weighting:**\n\nGWRF incorporates geographical information, which can be a double-edged sword. While it aims to capture spatial variations in the data, if the geographical weighting is not appropriately tuned or if the spatial relationships are not strong enough, it may introduce noise rather than useful information. This could lead to less reliable predictions, especially if the geographical factors do not significantly influence the target variable.\n\n### **4. Data Quality and Feature Selection:**\n\nThe performance of any model, including GWRF, heavily depends on the quality of the input data. If there are issues such as missing values, outliers, or irrelevant features in the dataset, these can negatively impact the model's performance. Additionally, if the features selected for the GWRF model do not adequately capture the underlying relationships in the data, it may struggle to make accurate predictions.\n\n# **Conclusion:**\n\nIn summary, the GWRF model's underperformance compared to the other models could be due to a combination of factors, including overfitting, insufficient number of trees, challenges with geographical weighting, and data quality issues. Addressing these aspects could help improve the model's performance and make it more competitive with the other models.\n\nThough because the asignment was made on a laptop with an Intel core i3 with 8GB of RAM increasing the number of trees or observations in training data was impossible because it would crash the laptop.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n",
    "supporting": [
      "Take-Home_Exercise03_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}