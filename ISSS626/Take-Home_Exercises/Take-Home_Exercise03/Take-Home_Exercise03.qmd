---
title: "Take Home Exercise 3b: Predicting HDB Resale Prices with Geographically Weighted Machine Learning Methods"
author: "Pelle Knegjes"
date: "Aug 26 2024"
date-modified: "last-modified"
execute: 
  eval: true
  echo: true
  message: false
  freeze: true
---

## **Setting the Scene**

Housing is an essential component of household wealth worldwide. Buying a housing has always been a major investment for most people. The price of housing is affected by many factors. Some of them are global in nature such as the general economy of a country or inflation rate. Others can be more specific to the properties themselves. These factors can be further divided to structural and locational factors. Structural factors are variables related to the property themselves such as the size, fitting, and tenure of the property. Locational factors are variables related to the neighbourhood of the properties such as proximity to childcare centre, public transport service and shopping centre.

Conventional, housing resale prices predictive models were built by using Ordinary Least Square (OLS) method. However, this method failed to take into consideration that spatial autocorrelation and spatial heterogeneity exist in geographic data sets such as housing transactions. With the existence of spatial autocorrelation, the OLS estimation of predictive housing resale pricing models could lead to biased, inconsistent, or inefficient results (Anselin 1998). In view of this limitation, **Geographical Weighted Models** were introduced to better calibrate predictive models for housing resale prices.

## **The Task**

In this take-home exercise, you are required to calibrate a predictive model to predict HDB resale prices between July-September 2024 by using HDB resale transaction records in 2023.

## **The Data**

For the purpose of this take-home exercise, **HDB Resale Flat Prices** provided by [**Data.gov.sg**](https://isss626-ay2024-25aug.netlify.app/take-home_ex03b) should be used as the core data set. The study should focus on either three-room, four-room or five-room flat.

Below is a list of recommended predictors to consider. However, students are free to include other appropriate independent variables.

-   Structural factors

    -   Area of the unit

    -   Floor level

    -   Remaining lease

    -   Age of the unit

    -   Main Upgrading Program (MUP) completed (optional)

-   Locational factors

    -   Proxomity to CBD

    -   Proximity to eldercare

    -   Proximity to foodcourt/hawker centres

    -   Proximity to MRT

    -   Proximity to park

    -   Proximity to good primary school

    -   Proximity to shopping mall

    -   Proximity to supermarket

    -   Numbers of kindergartens within 350m

    -   Numbers of childcare centres within 350m

    -   Numbers of bus stop within 350m

    -   Numbers of primary school within 1km


## Data Wrangling

First we want to load the following packages to achieve our task:

- sf
- spdep
- GWmodel
- SpatialML
- tmap
- rsample
- Metrics
- tidyverse

```{r}
pacman::p_load(sf, spdep, GWmodel, SpatialML, 
               tmap, rsample, Metrics, tidyverse, ClustGeo, 
               ggpubr, cluster, factoextra, NbClust,
               heatmaply, corrplot, psych, GGally, spdep, tmap, sfdep, plotly, Kendall, SpatialAcc, ggstatsplot, reshape2,httr, jsonlite, rvest)
```



```{r}
resale <- read_csv("data/aspatial/ResaleflatpricesbasedonregistrationdatefromJan-2017onwards.csv") %>%
  filter(month >= "2023-01" & month <= "2024-09")
```

```{r}
resale_tidy <- resale %>%
  mutate(address = paste(block,street_name)) %>%
  mutate(remaining_lease_yr = as.integer(
    str_sub(remaining_lease, 0, 2)))%>%
  mutate(remaining_lease_mth = as.integer(
    str_sub(remaining_lease, 9, 11)))
```

```{r}
#| eval: false
resale_selected <- resale_tidy #%>%
  #filter(month == "2024-09")
```

```{r}
#| eval: false
add_list <- sort(unique(resale_selected$address))
```

```{r}
#| eval: false
get_coords <- function(add_list){
  
  # Create a data frame to store all retrieved coordinates
  postal_coords <- data.frame()
    
  for (i in add_list){
    #print(i)

    r <- GET('https://www.onemap.gov.sg/api/common/elastic/search?',
           query=list(searchVal=i,
                     returnGeom='Y',
                     getAddrDetails='Y'))
    data <- fromJSON(rawToChar(r$content))
    found <- data$found
    res <- data$results
    
    # Create a new data frame for each address
    new_row <- data.frame()
    
    # If single result, append 
    if (found == 1){
      postal <- res$POSTAL 
      lat <- res$LATITUDE
      lng <- res$LONGITUDE
      new_row <- data.frame(address= i, 
                            postal = postal, 
                            latitude = lat, 
                            longitude = lng)
    }
    
    # If multiple results, drop NIL and append top 1
    else if (found > 1){
      # Remove those with NIL as postal
      res_sub <- res[res$POSTAL != "NIL", ]
      
      # Set as NA first if no Postal
      if (nrow(res_sub) == 0) {
          new_row <- data.frame(address= i, 
                                postal = NA, 
                                latitude = NA, 
                                longitude = NA)
      }
      
      else{
        top1 <- head(res_sub, n = 1)
        postal <- top1$POSTAL 
        lat <- top1$LATITUDE
        lng <- top1$LONGITUDE
        new_row <- data.frame(address= i, 
                              postal = postal, 
                              latitude = lat, 
                              longitude = lng)
      }
    }

    else {
      new_row <- data.frame(address= i, 
                            postal = NA, 
                            latitude = NA, 
                            longitude = NA)
    }
    
    # Add the row
    postal_coords <- rbind(postal_coords, new_row)
  }
  return(postal_coords)
}
```

```{r}
#| eval: false
coords <- get_coords(add_list)
```

```{r}
#| eval: false
write_rds(coords, "data/rds/coords.rds")
```

```{r}
coords <- read_rds("data/rds/coords.rds")
```


```{r}
resaleRAW <- left_join(resale_tidy, coords)

```


```{r}
resale <- st_as_sf(resaleRAW, 
                       coords = c("longitude", "latitude"),
                       crs=4326) %>%
  st_transform(crs = 3414)%>%
  st_jitter(amount = 0.5)
  


tmap_mode("plot")
#tm_shape(resale)+
 # tm_dots()


```

```{r}
buffer350 = st_buffer(resale, dist = 350)
buffer1000 = st_buffer(resale, dist = 1000)
```






# Loading in locational predictor variables
::: panel-tabset

## Proximity to CBD
```{r}
CBD <- data.frame(
  longitude = c(103.8503),  # Example longitudes
  latitude = c(1.2812)      # Example latitudes
)

CBD <- st_as_sf(CBD, 
                       coords = c("longitude", "latitude"),
                       crs=4326) %>%
  st_transform(crs = 3414)

```

```{r}
distances = st_distance(resale, CBD)
min_distances <- apply(distances, 1, min)
resale$PROX_CBD = min_distances



```

## Proximity to Eldercare
```{r}
eldercare <- st_read(dsn = "data/geospatial", layer = "ELDERCARE") %>%
  st_transform(crs = 3414) 

distances = st_distance(resale, eldercare)

min_distances <- apply(distances, 1, min)
resale$PROX_ELDERLYCARE = min_distances


```

## Proximity to Hawker centres
```{r}
hawker <- st_read("data/geospatial/HawkerCentresGEOJSON.geojson") %>%
  st_transform(crs = 3414) 

distances = st_distance(resale, hawker)

min_distances <- apply(distances, 1, min)
resale$PROX_HAWKER = min_distances

```

## Proximity to MRT

```{r}
mrt <- read_csv("data/aspatial/MRT.csv")

```
```{r}
mrt <- st_as_sf(mrt, 
                       coords = c("Longitude", "Latitude"),
                       crs=4326) %>%
  st_transform(crs = 3414)

```

```{r}
distances = st_distance(resale, mrt)

min_distances <- apply(distances, 1, min)
resale$PROX_MRT = min_distances


```

## Proximity to Park
```{r}
park <- st_read("data/geospatial/Parks.geojson")  %>%
  st_transform(crs = 3414)

distances = st_distance(resale, park)

min_distances <- apply(distances, 1, min)
resale$PROX_PARK = min_distances

```

##Proximity to Shopping Mall

```{r}
malls <- read_csv("data/aspatial/shopping_mall_coordinates.csv")

```
```{r}
malls <- st_as_sf(malls, 
                       coords = c("LONGITUDE", "LATITUDE"),
                       crs=4326) %>%
  st_transform(crs = 3414)

distances = st_distance(resale, malls)

min_distances <- apply(distances, 1, min)
resale$PROX_MALL = min_distances




```


## Proximity to Supermarket
```{r}
supermarket <- st_read("data/geospatial/SupermarketsGEOJSON.geojson") %>%
  st_transform(crs = 3414) 

distances = st_distance(resale, supermarket)

min_distances <- apply(distances, 1, min)
resale$PROX_SUPERMARKET = min_distances


```

## Number of Kidergartens within 350m
```{r}
kindergarten <- st_read("data/geospatial/Kindergartens.geojson") %>%
  st_transform(crs = 3414)

resale$WITHIN_350M_KINDERGARTEN = lengths(
  st_intersects(buffer350, kindergarten)
)


```

## Number of childcare centres within 350m
```{r}
childcare <- st_read("data/geospatial/ChildCareServices.geojson") %>%
  st_transform(crs = 3414) 


resale$WITHIN_350M_CHILDCARE = lengths(
  st_intersects(buffer350, childcare)
)


```

## Numbers of bus stop within 350m
```{r}
busstop <- st_read(dsn = "data/geospatial", layer = "BusStop") %>%
  st_transform(crs = 3414) 

resale$WITHIN_350M_BUS = lengths(
  st_intersects(buffer350, busstop)
)


```

## Numbers of primary school within 1km
```{r}
schools <- read_csv("data/aspatial/Generalinformationofschools.csv") %>%
  filter(mainlevel_code == "PRIMARY")
```

```{r}
#| eval: false
add_list1 <- sort(unique(schools$address))
```

```{r}
#| eval: false
get_coords1 <- function(add_list1){
  
  # Create a data frame to store all retrieved coordinates
  postal_coords1 <- data.frame()
    
  for (i in add_list1){
    #print(i)

    r <- GET('https://www.onemap.gov.sg/api/common/elastic/search?',
           query=list(searchVal=i,
                     returnGeom='Y',
                     getAddrDetails='Y'))
    data <- fromJSON(rawToChar(r$content))
    found <- data$found
    res <- data$results
    
    # Create a new data frame for each address
    new_row <- data.frame()
    
    # If single result, append 
    if (found == 1){
      postal <- res$POSTAL 
      lat <- res$LATITUDE
      lng <- res$LONGITUDE
      new_row <- data.frame(address= i, 
                            postal = postal, 
                            latitude = lat, 
                            longitude = lng)
    }
    
    # If multiple results, drop NIL and append top 1
    else if (found > 1){
      # Remove those with NIL as postal
      res_sub <- res[res$POSTAL != "NIL", ]
      
      # Set as NA first if no Postal
      if (nrow(res_sub) == 0) {
          new_row <- data.frame(address= i, 
                                postal = NA, 
                                latitude = NA, 
                                longitude = NA)
      }
      
      else{
        top1 <- head(res_sub, n = 1)
        postal <- top1$POSTAL 
        lat <- top1$LATITUDE
        lng <- top1$LONGITUDE
        new_row <- data.frame(address= i, 
                              postal = postal, 
                              latitude = lat, 
                              longitude = lng)
      }
    }

    else {
      new_row <- data.frame(address= i, 
                            postal = NA, 
                            latitude = NA, 
                            longitude = NA)
    }
    
    # Add the row
    postal_coords1 <- rbind(postal_coords1, new_row)
  }
  return(postal_coords1)
}
```

```{r}
#| eval: false
coordsschool <- get_coords(add_list1)
```

```{r}
#| eval: false
write_rds(coordsschool, "data/rds/coordsschool.rds")
```
```{r}
coordsschool <- read_rds("data/rds/coordsschool.rds")
```


```{r}
schools <- left_join(schools, coordsschool)

```
```{r}
schools <- st_as_sf(schools, 
                       coords = c("longitude", "latitude"),
                       crs=4326) %>%
  st_transform(crs = 3414)
```

```{r}
resale$WITHIN_1KM_SCHOOL = lengths(
  st_intersects(buffer1000, schools)
)

```

## Proximity to Good Primary School
```{r}
goodschools = c("AI TONG SCHOOL", "ANGLO-CHINESE SCHOOL (JUNIOR)", "ANGLO-CHINESE SCHOOL (PRIMARY)","CATHOLIC HIGH SCHOOL (PRIMARY SECTION)", "CHIJ ST. NICHOLAS GIRLS’ SCHOOL (PRIMARY SECTION)", "CHONGFU SCHOOL",
"FAIRFIELD METHODIST SCHOOL (PRIMARY)", "GONGSHANG PRIMARY SCHOOL",
"HENRY PARK PRIMARY SCHOOL", "HOLY INNOCENTS' PRIMARY SCHOOL",
"HORIZON PRIMARY SCHOOL", "METHODIST GIRLS' SCHOOL (PRIMARY)","NAN HUA PRIMARY SCHOOL", "NANYANG PRIMARY SCHOOL",
"NORTHLAND PRIMARY SCHOOL", "PEI CHUN PUBLIC SCHOOL",
"PEI HWA PRESBYTERIAN PRIMARY SCHOOL", "RED SWASTIKA SCHOOL",
"ROSYTH SCHOOL", "RULANG PRIMARY SCHOOL", "SOUTH VIEW PRIMARY SCHOOL", "ST. HILDA'S PRIMARY SCHOOL", "ST. JOSEPH'S INSTITUTION JUNIOR", "TAO NAN SCHOOL", "TEMASEK PRIMARY SCHOOL")

good_schools = schools %>%
  filter(school_name %in% goodschools)


distances = st_distance(resale, good_schools)

min_distances <- apply(distances, 1, min)
resale$PROX_GOOD_PRISCH = min_distances

```


:::

GOtta use st_distance for the proximity

using buffering for within 350

how to count point features wihtin a certain distance?

USE SPACIAL ML random forrests use 50 trees
if we wanna make own os map r5r pack



```{r}
mpsz = st_read(dsn = "data/geospatial", layer = "MP14_SUBZONE_WEB_PL")%>%
  st_transform(crs = 3414)

```

# tidiy-ing data
```{r}
resale_tidy = resale %>%
  select(floor_area_sqm,  resale_price, remaining_lease_yr,PROX_CBD, PROX_ELDERLYCARE, PROX_HAWKER, PROX_MRT, PROX_PARK, PROX_GOOD_PRISCH, PROX_MALL, PROX_SUPERMARKET, WITHIN_350M_KINDERGARTEN, WITHIN_350M_CHILDCARE, WITHIN_350M_BUS, WITHIN_1KM_SCHOOL,month, flat_type,flat_model, storey_range, lease_commence_date, remaining_lease_mth, address, remaining_lease, ,postal, geometry)



```





# Data Sampling

## Creating subsets
```{r}
# Filter for data from the year 2023
subset_2023 <- resale_tidy %>%
  filter(grepl("^2023-", month) & flat_type %in% c("3 ROOM", "4 ROOM", "5 ROOM"))

# Filter for data between July and September 2024
subset_jul_sep_2024 <- resale_tidy %>%
  filter(month %in% c("2024-07", "2024-08", "2024-09") & flat_type %in% c("3 ROOM", "4 ROOM", "5 ROOM"))
  
```

##splitting the training data
Since we will train the model based on 2023 data we will split this data into a training and test data sets with 65% and 35% respectively by using initial_split() of the rsample package.

```{r}
set.seed(1234)
resale_split <- initial_split(subset_2023, 
                              prop = 6.5/10,)
train_data <- training(resale_split)
test_data <- testing(resale_split)
```


```{r}
write_rds(train_data, "data/rds/train_data.rds")
write_rds(test_data, "data/rds/test_data.rds")
```

# Computing Correlation Matrix
Before loading the predictors into a predictive model, it is always a good practice to use correlation matrix to examine if there is sign of multicolinearity.

```{r}
#| fig-width: 12
#| fig-height: 10
nogeo_2023 <- subset_2023 %>%
  st_drop_geometry()
corrplot::corrplot(cor(nogeo_2023[, 1:15]), 
                   diag = FALSE, 
                   order = "AOE",
                   tl.pos = "td", 
                   tl.cex = 0.5, 
                   method = "number", 
                   type = "upper")
```

The correlation matrix above shows that all the correlation values are below 0.8. Hence, there is no sign of multicolinearity.

# Retriving the Stored Data

```{r}
train_data <- read_rds("data/rds/train_data.rds")
test_data <- read_rds("data/rds/test_data.rds")
```

# Building a non-spatial multiple linear regression

```{r}
#| eval: false
price_mlr <- lm(resale_price ~ floor_area_sqm + remaining_lease_yr +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_GOOD_PRISCH +PROX_MALL +                   PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_SCHOOL,
                data=train_data)
summary(price_mlr)
```



```{r}
#| eval: false
write_rds(price_mlr, "data/rds/price_mlr.rds" ) 
```

# gwr predictive method
In this section, we will calibrate a model to predict the resale price by using geographically weighted regression methods of the GWmodel package.

## Converting the sf data.frame to SpatialPointDataFrame
```{r}
train_data_sp <- as_Spatial(train_data)
train_data_sp
```


## Computing adaptive bandwidth
Next, bw.gwr() of GWmodel package will be used to determine the optimal bandwidth to be used.
```{r}
#| eval: false
bw_adaptive <- bw.gwr(resale_price ~ floor_area_sqm +
                  remaining_lease_yr +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_GOOD_PRISCH + PROX_MALL                 + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_SCHOOL,
                  data=train_data_sp,
                  approach="CV",
                  kernel="gaussian",
                  adaptive=TRUE,
                  longlat=FALSE)
```



```{r}
#| eval: false
write_rds(bw_adaptive, "data/rds/bw_adaptive.rds")
```


## Constructing the adaptive bandwidth gwr model
```{r}
bw_adaptive <- read_rds("data/rds/bw_adaptive.rds")
```




```{r}
#| eval: false
gwr_adaptive <- gwr.basic(formula = resale_price ~
                            floor_area_sqm +
                            remaining_lease_yr + PROX_CBD + 
                            PROX_ELDERLYCARE + PROX_HAWKER +
                            PROX_MRT + PROX_PARK + PROX_GOOD_PRISCH+ PROX_MALL + 
                            PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +  WITHIN_1KM_SCHOOL+                           WITHIN_350M_CHILDCARE + WITHIN_350M_BUS,
                          data=train_data_sp,
                          bw=bw_adaptive, 
                          kernel = 'gaussian', 
                          adaptive=TRUE,
                          longlat = FALSE)
```


```{r}
#| eval: false
write_rds(gwr_adaptive, "data/rds/gwr_adaptive.rds")
```


```{r}
gwr_adaptive <- read_rds("data/rds/gwr_adaptive.rds")

```


```{r}
gwr_adaptive

```

## Converting the test data from sf data.frame to SpatialPointDataFrame

```{r}
test_data_sp <- test_data %>%
  as_Spatial()
test_data_sp

```


```{r}
#| eval: false
gwr_bw_test_adaptive <- bw.gwr(resale_price ~ floor_area_sqm +
                  remaining_lease_yr +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_GOOD_PRISCH + PROX_MALL                 + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_SCHOOL,
                  data=test_data_sp,
                  approach="CV",
                  kernel="gaussian",
                  adaptive=TRUE,
                  longlat=FALSE)

```
```{r}
#| eval: false
write_rds(gwr_bw_test_adaptive, "data/rds/gwr_bw_test_adaptive.rds")
```

```{r}
#| eval: false
gwr_pred <- gwr.predict(formula = resale_price ~
                          floor_area_sqm +
                  remaining_lease_yr +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_GOOD_PRISCH + PROX_MALL                 + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_SCHOOL, 
                        data=train_data_sp, 
                        predictdata = test_data_sp, 
                        bw=40, 
                        kernel = 'gaussian', 
                        adaptive=TRUE, 
                        longlat = FALSE)

```

# Preparing coordinates data
## Extracting coordinates data
```{r}
coords <- st_coordinates(subset_2023)
coords_train <- st_coordinates(train_data)
coords_test <- st_coordinates(test_data)

```


```{r}
#| eval: false
coords_train <- write_rds(coords_train, "data/rds/coords_train.rds" )
coords_test <- write_rds(coords_test, "data/rds/coords_test.rds" )
```
```{r}
coords_train = read_rds("data/rds/coords_train.rds")
coords_test <- read_rds("data/rds/coords_test.rds")
```

## Droping geometry field
```{r}
train_data <- train_data %>% 
  st_drop_geometry()

```

# Calibrating Random Forest Model

```{r}
set.seed(1234)
rf <- ranger(resale_price ~ floor_area_sqm +
                  remaining_lease_yr +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_GOOD_PRISCH + PROX_MALL                 + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_SCHOOL,
             data=train_data,
             num.trees = 50)
rf

```


```{r}
#| eval: false
write_rds(rf, "data/rds/rf.rds")

```


```{r}
rf <- read_rds("data/rds/rf.rds")
rf

```
# Calibrating Geographical Random Forest Model


## Calibrating using training data
```{r}
set.seed(1234)
gwRF_adaptive <- grf(formula = resale_price ~ floor_area_sqm +
                  remaining_lease_yr +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_GOOD_PRISCH + PROX_MALL                 + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_SCHOOL,
                     dframe=train_data, 
                     bw=55,
                     kernel="adaptive",
                     coords=coords_train,
                  ntree = 50)

```


```{r}
write_rds(gwRF_adaptive, "data/rds/gwRF_adaptive.rds")

```


```{r}
gwRF_adaptive <- read_rds("data/rds/gwRF_adaptive.rds")

```

## Predicting by using test data
Preparing the test data
```{r}
test_data <- cbind(test_data, coords_test) %>%
  st_drop_geometry()

```

###  Predicting with test data
```{r}
gwRF_pred <- predict.grf(gwRF_adaptive, 
                           test_data, 
                           x.var.name="X",
                           y.var.name="Y", 
                           local.w=1,
                           global.w=0)

```

```{r}
GRF_pred <- write_rds(gwRF_pred, "data/rds/GRF_pred.rds")

```

### Converting the predicting output into a data frame

```{r}
GRF_pred <- read_rds("data/rds/GRF_pred.rds")
GRF_pred_df <- as.data.frame(GRF_pred)

```
In the code chunk below, cbind() is used to append the predicted values onto test_datathe

```{r}
test_data_p <- cbind(test_data, GRF_pred_df)

```


```{r}

write_rds(test_data_p, "data/rds/test_data_p.rds")
```

## Calculating Root Mean Square Error

```{r}
rmse(test_data_p$resale_price, 
     test_data_p$GRF_pred)

```

## Visualising the predicted values
```{r}
ggplot(data = test_data_p,
       aes(x = GRF_pred,
           y = resale_price)) +
  geom_point()

```
A better predictive model should have the scatter point close to the diagonal line. The scatter plot can be also used to detect if any outliers in the model.




# Building Fixed Bandwidth GWR Model
```{r}
#|eval: false
bw_fixed <- bw.gwr(formula = resale_price ~ floor_area_sqm +
                  remaining_lease_yr +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_GOOD_PRISCH + PROX_MALL                 + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_SCHOOL, 
                   data=train_data_sp, 
                   approach="CV", 
                   kernel="gaussian", 
                   adaptive=FALSE, 
                   longlat=FALSE)

```
```{r}
#|eval: false
write_rds(bw_fixed, "data/rds/bw_fixed.rds")
```
```{r}
bw_fixed <- read_rds("data/rds/bw_fixed.rds")

```

# GWModel method - fixed bandwith

Now we can use the code chunk below to calibrate the gwr model using fixed bandwidth and gaussian kernel.

```{r}
#|eval: false
gwr_fixed <- gwr.basic(formula = resale_price ~ floor_area_sqm +
                  remaining_lease_yr +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_GOOD_PRISCH + PROX_MALL                 + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_SCHOOL, 
                       data=train_data_sp, 
                       bw=bw_fixed, 
                       kernel = 'gaussian', 
                       longlat = FALSE)
```

The output is saved in a list of class “gwrm”. The code below can be used to display the model output.


```{r}
#|eval: false
write_rds(gwr_fixed, "data/rds/gwr_fixed.rds")
```

```{r}
gwr_fixed <- read_rds("data/rds/gwr_fixed.rds")

```

```{r}
#|eval: false
gwr_fixed
```
# Building Adaptive Bandwidth GWR Model

## Computing the adaptive bandwidth
Similar to the earlier section, we will first use bw.gwr() to determine the recommended data point to use.
```{r}
#|eval: false
bw_adaptive <- bw.gwr(formula = resale_price ~ floor_area_sqm +
                  remaining_lease_yr +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_GOOD_PRISCH + PROX_MALL                 + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_SCHOOL, 
                      data=train_data_sp, 
                      approach="CV", 
                      kernel="gaussian", 
                      adaptive=TRUE, 
                      longlat=FALSE)
```

```{r}
#|eval: false
write_rds(bw_adaptive, "data/rds/bw_adaptive.rds")
```

```{r}
bw_adaptive <- read_rds("data/rds/bw_adaptive.rds")

```

```{r}

bw_adaptive
```

## Constructing the adaptive bandwidth gwr model
Now, we can go ahead to calibrate the gwr-based model by using adaptive bandwidth and gaussian kernel as shown in the code chunk below.

```{r}
#|eval: false
gwr_adaptive <- gwr.basic(formula = resale_price ~ floor_area_sqm +
                  remaining_lease_yr +
                  PROX_CBD + PROX_ELDERLYCARE + PROX_HAWKER +
                  PROX_MRT + PROX_PARK + PROX_GOOD_PRISCH + PROX_MALL                 + PROX_SUPERMARKET + WITHIN_350M_KINDERGARTEN +
                  WITHIN_350M_CHILDCARE + WITHIN_350M_BUS +
                  WITHIN_1KM_SCHOOL, 
                      data=train_data_sp, 
                          bw=bw_adaptive, 
                          kernel = 'gaussian', 
                          adaptive=TRUE, 
                          longlat = FALSE)
```

```{r}
#|eval: false
write_rds(gwr_adaptive, "data/rds/gwr_adaptive.rds")
```

```{r}
gwr_adaptive <- read_rds("data/rds/gwr_adaptive.rds")

```

The code below can be used to display the model output.
```{r}
gwr_adaptive
```

LOOK HERE
The report shows that the AICc the adaptive distance gwr is 87329.3 which is even smaller than the AICc of the fixed distance gwr of *42263.61******

# Visualising GWR Output

## Converting SDF into sf data.frame
To visualise the fields in SDF, we need to first covert it into sf data.frame by using the code chunk below.

```{r}
gwr_adaptive_output <- as.data.frame(
  gwr_adaptive$SDF) %>%
  select(-c(2:15))
```


```{r}
gwr_sf_adaptive <- cbind(train_data_sp,
                         gwr_adaptive_output)
```


```{r}
glimpse(gwr_sf_adaptive)
```

```{r}
summary(gwr_adaptive$SDF$yhat)
```

## Visualising local R2
```{r}
tmap_mode("plot")
tmap_options(check.and.fix = TRUE)
tm_shape(mpsz)+
  tm_polygons(alpha = 0.1) +
tm_shape(gwr_sf_adaptive) +  
  tm_dots(col = "Local_R2",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))
```


## Visualising coefficient estimates
The code chunks below is used to create an interactive point symbol map.

```{r}
tmap_options(check.and.fix = TRUE)
tmap_mode("plot")
#AREA_SQM_SE <- 
tm_shape(mpsz)+
  tm_polygons(alpha = 0.1) +
tm_shape(gwr_sf_adaptive) +  
  tm_dots(col = "AREA_SQM_SE",
          border.col = "gray",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

#AREA_SQM_TV <- 
  tm_shape(mpsz)+
  tm_polygons(alpha = 0.1) +
tm_shape(gwr_sf_adaptive) +  
  tm_dots(col = "AREA_SQM_TV",
          border.col = "gray",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))

#tmap_arrange(AREA_SQM_SE, AREA_SQM_TV, 
 #            asp=1, ncol=2,
  #           sync = TRUE)
```


```{r}

```



```{r}

```



```{r}

```



```{r}

```


```{r}

```



```{r}

```


```{r}

```


```{r}

```



```{r}

```


```{r}

```


```{r}

```



```{r}

```

```{r}

```


```{r}

```



```{r}

```


```{r}

```



```{r}

```



```{r}

```



```{r}

```


```{r}

```




```{r}

```


```{r}

```


```{r}

```



```{r}

```


```{r}

```


```{r}

```



```{r}

```

```{r}

```


```{r}

```



```{r}

```


```{r}

```



```{r}

```



```{r}

```



```{r}

```


```{r}

```




```{r}

```


```{r}

```


```{r}

```



```{r}

```


```{r}

```


```{r}

```



```{r}

```

```{r}

```


```{r}

```



```{r}

```


```{r}

```



```{r}

```



```{r}

```



```{r}

```


```{r}

```



```{r}

```


```{r}

```


```{r}

```



```{r}

```


```{r}

```


```{r}

```



```{r}

```

```{r}

```


```{r}

```



```{r}

```


```{r}

```



```{r}

```



```{r}

```



```{r}

```


```{r}

```
